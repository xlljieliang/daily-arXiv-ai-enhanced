<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文提出PolypSeg-GradCAM，一种可解释的深度学习框架，结合U-Net和Grad-CAM进行透明化的息肉分割，在Kvasir-SEG数据集上取得优异性能（IoU 0.9257），为AI辅助结肠镜检查提供可靠解决方案。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要癌症死因，胃肠道息肉是其关键前兆。早期准确分割对预防CRC进展至关重要，但手动分割劳动密集且存在观察者差异。现有深度学习方法缺乏可解释性，阻碍临床采用。

Method: 提出PolypSeg-GradCAM框架，集成U-Net架构和梯度加权类激活映射(Grad-CAM)，在Kvasir-SEG数据集（1000张标注内窥镜图像）上训练评估。

Result: 实验结果显示稳健的分割性能，测试集平均IoU达0.9257，训练和验证集Dice系数均高于0.96。Grad-CAM可视化证实预测基于临床相关区域。

Conclusion: PolypSeg-GradCAM将高分割精度与可解释性结合，向可靠、可信的AI辅助结肠镜检查和改进的早期结直肠癌预防迈进一步。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE是一个基于深度学习的远程眼科应用，用于通过视网膜图像自动检测糖尿病视网膜病变，准确率达到85.4%，旨在改善偏远地区的医疗可及性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致成人视力丧失的主要原因，特别是在医疗资源匮乏地区，需要开发高效、可扩展的筛查解决方案。

Method: 使用多种卷积神经网络（ResNet-18、EfficientNet-B0和SqueezeNet）开发和评估系统，平衡准确性和计算效率，最终模型实现实时筛查。

Result: 系统分类疾病严重程度的准确率为85.4%，具备云可扩展性、安全数据管理和多用户框架，支持临床和远程医疗环境。

Conclusion: AI驱动的远程医疗解决方案有望扩大糖尿病视网膜病变筛查的覆盖范围，特别是在偏远和资源有限的环境中。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出了一种名为SIM（Self Identity Mapping）的数据内在正则化框架，通过逆映射机制增强表示学习，降低前向传播中的信息损失并改善梯度流动。


<details>
  <summary>Details</summary>
Motivation: 传统正则化技术通常依赖启发式方法，在不同设置下效果不稳定。需要一种更可靠有效的正则化方法来提升深度学习模型的泛化能力。

Method: SIM通过从变换后的输出重构输入来减少信息损失。为了提升计算效率，提出了ρSIM版本，采用补丁级特征采样和基于投影的方法重构潜在特征。

Result: 在图像分类、少样本提示学习和领域泛化等任务上，ρSIM相比基线方法取得了持续改进。该方法与现有正则化方法正交，能进一步提升其效果。

Conclusion: SIM是一种模型无关、任务无关的正则化器，可作为即插即用模块应用于不同网络架构和任务，在视觉和非视觉领域都表现出色。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA是一种基于动量的自适应梯度反演攻击方法，能够在单轮平均梯度SAG机制下实现高保真多图像重建，特别是在大批量场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究在具有挑战性的单轮平均梯度SAG机制下的梯度反演问题，其中每个样本的线索都纠缠在单个批次平均梯度中，需要开发新的方法来感知潜在的每图像信号。

Method: MAGIA框架包含两个核心创新：1）封闭形式的组合重缩放，创建可证明更紧的优化边界；2）基于动量的整批次和子集损失混合，确保重建鲁棒性。该方法通过探测随机数据子集来感知潜在图像信号。

Result: 大量实验表明，MAGIA显著优于先进方法，在大批量场景中实现高保真多图像重建，而先前的方法在此类场景中失败。

Conclusion: MAGIA在计算足迹与标准求解器相当且无需任何辅助信息的情况下，成功解决了单轮平均梯度SAG机制下的梯度反演挑战。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer是一个专门针对阿拉伯文档OCR的视觉语言模型，通过大规模数据集和解码器微调策略，显著优于现有解决方案，在阿拉伯文档OCR领域达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯文档OCR由于草书字体、多样化字体、变音符和从右到左的书写方向而具有挑战性。现有的多模态大语言模型在高资源语言上表现良好，但在阿拉伯语上性能有限。

Method: 使用结合合成和真实世界文档的大规模数据集，采用解码器微调策略对预训练的MLLM进行适配，同时保留通用视觉特征。还提出了Misraj-DocOCR基准用于评估。

Result: Baseer显著优于现有的开源和商业解决方案，实现了0.25的WER，在阿拉伯文档OCR领域建立了新的最先进水平。

Conclusion: 领域特定的通用MLLM适配具有显著优势，为像阿拉伯语这样的形态丰富语言的高精度OCR建立了强有力的基准。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度学习框架，将稀疏的InSAR时间序列数据转换为密集的时空张量，首次实现了先进计算机视觉架构在形变预测问题上的直接应用。


<details>
  <summary>Details</summary>
Motivation: 监测地面位移对城市基础设施稳定性和减轻地质灾害至关重要，但基于稀疏InSAR时间序列数据预测未来形变仍是一个重大挑战。

Method: 设计并实现了混合CNN-LSTM模型，专门用于同时学习生成数据张量的空间模式和时间依赖性，并与LightGBM和LASSO回归等机器学习基线进行对比。

Result: 使用爱尔兰东部的Sentinel-1数据验证，结果表明所提架构能提供更准确和空间一致的预测，为该任务建立了新的性能基准。

Conclusion: 可解释性分析显示基线模型往往采用简单的持续性模式，强调了集成时空方法捕捉地面形变复杂动态的必要性，证实了时空深度学习在高分辨率形变预测中的有效性和潜力。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: Scrapbook框架是一种生成大规模数据集的新方法，用于探究AI模型学习到的概念，重点关注物体识别、位置关系和属性识别等基础概念。


<details>
  <summary>Details</summary>
Motivation: 需要验证AI模型对基本概念的理解能力，为处理更复杂任务奠定基础。现有模型在位置理解和约束性问题方面存在挑战。

Method: 通过生成包含大量关于单个概念问题和广泛语言变体的数据集，系统评估模型对基础元素的理解能力。

Result: 当代模型在物体识别方面表现良好，但在理解位置信息和处理带约束的问题时存在困难。MobileVLM-V2模型出现显著答案分歧，其他模型存在肯定答案偏见，在几何形状和位置信息问题上表现不佳。

Conclusion: Scrapbook框架为生成多样化数据集提供了有价值的工具，可用于系统评估和提升AI模型性能，特别是在理解一致性和位置关系方面需要改进。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文通过实证分析量化了视觉-语言-视觉管道中的信息损失，发现在描述-生成瓶颈中，99.3%的样本存在显著感知退化，91.5%存在显著结构信息损失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流中的集成增加，理解视觉-语言-视觉管道中的信息损失对于评估系统局限性变得重要，但目前通过文本中介传递视觉内容时的退化程度尚未得到充分量化。

Method: 生成150对通过描述-生成管道的图像对，应用现有指标（LPIPS、SSIM和颜色距离）来测量感知、结构和色彩维度上的信息保存情况。

Result: 评估显示99.3%的样本表现出显著的感知退化，91.5%的样本表现出显著的结构信息损失。

Conclusion: 描述-生成瓶颈代表了当代多模态系统中可测量且一致的限制，提供了该瓶颈存在显著信息损失的实证证据。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 本文提出了一种AI驱动的工作流程，利用高分辨率卫星影像自动推断屋顶属性，以解决小岛屿发展中国家建筑结构数据缺失问题，为灾害风险评估提供支持。


<details>
  <summary>Details</summary>
Motivation: 小岛屿发展中国家（特别是加勒比地区）缺乏详细的建筑结构信息，这限制了灾害风险评估和城市韧性规划的能力。

Method: 比较了地理空间基础模型结合浅层分类器与微调深度学习模型在屋顶分类中的效果，并评估了引入邻国训练数据对模型性能的影响。

Result: 最佳模型在屋顶坡度和屋顶材料分类上分别达到了0.88和0.83的F1分数。

Conclusion: 结合本地能力建设，该工作为小岛屿发展中国家提供了利用AI和地球观测数据实现更高效、基于证据的城市治理的新能力。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 该论文提出了VLA-LPAF轻量级模块来解决视觉-语言-动作模型中视角异构性问题，通过仅使用2D数据实现视角自适应，显著提升了模型在多个基准测试上的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在处理来自不同视角的视觉观察时存在视角异构性问题，这限制了模型的泛化能力。不同环境中第三方全局摄像头和手腕局部摄像头捕获的视觉特征差异显著，需要解决这种视角不一致性。

Method: 提出了轻量级模块VLA-LPAF，使用单视角图像进行微调，在潜在空间中融合多视角观察，有效且高效地弥合视角不一致性造成的差距。基于RoboFlamingo模型构建了RoboFlamingo-LPAF框架。

Result: 实验表明，RoboFlamingo-LPAF在CALVIN基准上平均提升约8%的任务成功率，在LIBERO上提升15%，在定制化仿真基准上提升30%。真实世界任务也验证了模型具有视角自适应特性。

Conclusion: VLA-LPAF模块能够有效解决VLA模型中的视角异构性问题，通过轻量级的2D数据训练实现多视角适应，显著提升了模型在各种环境下的性能和泛化能力。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件相机的立体深度估计方法URNet，通过局部-全局精炼模块和KL散度不确定性建模，在DSEC数据集上超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围和低延迟的优势，但现有的基于事件相机的立体深度估计方法在精度和可靠性方面仍有提升空间。

Method: 提出了URNet网络，包含局部-全局精炼模块来捕捉细粒度局部细节和长距离全局上下文，并引入基于KL散度的不确定性建模方法来增强预测可靠性。

Result: 在DSEC数据集上的大量实验表明，URNet在定性和定量评估中都一致优于最先进的方法。

Conclusion: URNet通过有效的局部-全局特征融合和不确定性建模，显著提升了事件相机立体深度估计的性能和可靠性。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一个用于从多梯度DWI和形态学MRI数据中识别外周神经系统的新型混合AI框架，通过模糊空间关系编码解剖知识，无需手动选择ROI，在子宫内膜异位症患者的腰骶丛神经识别中表现优于标准纤维束成像。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛和可能的神经受累，但外周神经成像仍然是一个挑战。需要开发能够自动识别外周神经系统的技术来改善诊断。

Method: Visionerves采用两阶段混合AI框架：(A)使用深度学习模型自动分割解剖结构；(B)通过符号空间推理进行纤维束成像和神经识别。该方法通过模糊空间关系编码解剖知识，无需手动选择感兴趣区域。

Result: 在10名子宫内膜异位症（确诊或疑似）女性的腰骶丛神经应用中，Visionerves相比标准纤维束成像有显著改进，Dice评分提高高达25%，空间误差减少到小于5毫米。

Conclusion: 这种自动且可重复的方法能够进行详细的神经分析，为子宫内膜异位症相关神经病变以及其他涉及神经的疾病的非侵入性诊断铺平了道路。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive是首个在巴基斯坦驾驶环境中收集的隐私保护多模态驾驶员行为数据集，结合智能手机惯性/GPS传感器数据和同步的路面视频，记录三种驾驶行为（正常、攻击性、危险）


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集主要来自发达国家，缺乏新兴经济体行为多样性代表的问题，同时保护驾驶员面部隐私

Method: 使用定制Android应用采集高频率加速度计、陀螺仪、GPS数据与连续视频，所有数据源精确时间对齐，涵盖多种道路类型和驾驶场景

Result: 构建了包含原始、处理和语义层的结构化数据集，为驾驶员行为分类、交通安全分析和ADAS开发提供适配性

Conclusion: V-SenseDrive填补了全球驾驶员行为数据集的关键空白，为情境感知智能交通解决方案奠定了基础

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL是一个多模态大语言模型系列（3B-70B参数），通过创新的领域增强技术实现SOTA性能，在OCR、文档理解和数学推理等任务上表现优异，验证了大规模AI基础设施的训练能力。


<details>
  <summary>Details</summary>
Motivation: 开发领域增强的多模态模型，使其在保持通用性能的同时，在特定领域（如OCR、文档理解）具有显著优势，适合企业部署场景。

Method: 采用多阶段渐进式训练和高精度数据合成流水线，结合长链思维推理能力，在百度昆仑P800芯片上进行大规模训练。

Result: 在通用基准测试中与领先开源模型相当，在CCBench、SEEDBench IMG、ScienceQA、MMStar等基准上达到SOTA性能，OCRBench 873分，DocVQA 94.75%，MathVista 78.6%。

Conclusion: 该工作建立了开发领域增强多模态模型的有效方法学，验证了大规模AI基础设施训练SOTA级多模态模型的能力，具有90%以上的扩展效率。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow是一个基于ODE的去雾框架，将大气散射模型重新表述为常微分方程，通过单步推理实现真实世界图像去雾，并利用马尔可夫链布朗运动生成非均匀雾霾数据来解决训练数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法因缺乏配对的真实世界训练数据和领域差距，难以泛化到真实场景；传统基于大气散射模型的物理方法在处理真实世界复杂性和多样化雾霾模式方面存在不足。

Method: 提出HazeFlow框架，将大气散射模型重新表述为ODE，借鉴Rectified Flow思想学习从雾霾图像到清晰图像的最优ODE轨迹；引入基于马尔可夫链布朗运动的非均匀雾霾生成方法模拟真实雾霾模式。

Result: 在多个真实世界去雾基准数据集上，HazeFlow实现了最先进的性能表现。

Conclusion: HazeFlow通过ODE框架和真实雾霾模拟方法，有效解决了真实世界图像去雾的挑战，在泛化性和性能方面均有显著提升。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种压缩版EcoWeedNet模型，通过结构化通道剪枝、量化感知训练和TensorRT加速，在Jetson Orin Nano上实现高效农业杂草检测


<details>
  <summary>Details</summary>
Motivation: 农业边缘设备资源有限，需要轻量化的深度学习模型部署方案

Method: 采用结构化通道剪枝处理复杂架构（残差连接、注意力机制、拼接操作和CSP块），结合量化感知训练和TensorRT加速

Result: 模型大小减少68.5%，计算量减少3.2 GFLOPs，推理速度达184 FPS（FP16），比基线快28.7%。在CottonWeedDet12数据集上，39.5%剪枝率的模型性能优于YOLO11n和YOLO12n，达到83.7%精确率、77.5%召回率和85.9% mAP50

Conclusion: 该方法证明了在保持高精度的同时显著提升效率，适用于精准农业应用

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出一种新颖的多模态学习框架，通过增强模态dropout和对比学习解决模态不平衡和缺失问题，在临床疾病检测任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗诊断越来越多使用多模态数据，需要模型能够有效融合异构信息并保持对缺失模态的鲁棒性。现实应用中存在模态不平衡和缺失的挑战。

Method: 引入可学习的模态token来改进缺失感知的模态融合，并在传统单模态对比目标基础上增加融合多模态表示的对比学习。

Result: 在大规模临床数据集上的实验表明，该方法在单模态可用等实际场景中达到最先进性能，并能成功与CT基础模型集成。

Conclusion: 该方法为多模态学习提供了有效、高效且可推广的解决方案，具有显著的现实临床应用潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究系统评估了9种CNN和ViT分割架构在肺栓塞(PE)分割任务上的性能，发现3D U-Net with ResNet encoder表现最佳，CNN模型普遍优于ViT，分类预训练反而会降低分割性能。


<details>
  <summary>Details</summary>
Motivation: 肺栓塞分割是临床诊断中的重要任务，但现有研究缺乏对不同深度学习架构在PE分割任务上的系统性性能评估和比较。

Method: 使用490个CTPA扫描的标注数据集，在统一测试框架下评估9种分割架构（CNN和ViT），比较预训练和随机初始化权重的影响。

Result: 最佳模型达到0.7131的Dice分数，在60个测试扫描中检测到181个栓子（49个假阳性，28个假阴性）。3D模型和CNN架构表现更优，分类预训练对分割任务有负面影响。

Conclusion: 3D U-Net with ResNet encoder是PE分割的有效架构，远端栓子分割仍具挑战性，PE分类和分割任务依赖不同的判别特征。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出了一种新颖的图神经网络，将时间动态与静态手形配置分离，用于手语手形识别，在37个手形类别上达到46%的准确率。


<details>
  <summary>Details</summary>
Motivation: 手形在手语中具有基础音系学作用，但计算方法很少显式建模手形，限制了识别准确性和语言分析。

Method: 结合解剖学启发的图结构和对比学习，解决手形识别中的关键挑战，包括细微的类间差异和时间变化。

Result: 建立了签名序列中结构化手形识别的首个基准，在37个手形类别上达到46%的准确率（基线方法为25%）。

Conclusion: 该方法显著提升了手形识别性能，为手语计算分析提供了更精确的工具。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文研究分类任务对胎儿超声图像中分布外检测性能的影响，发现不同分类任务对OOD检测效果差异显著，且最佳任务取决于ID-OOD标准类型（图像特征偏移或解剖特征偏移）。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注不确定性量化方法，但忽视了分类任务本身对OOD检测的影响。在胎儿超声图像存在异质性和不同临床设置的背景下，需要研究任务选择对可靠OOD检测的重要性。

Method: 在四个分类任务上评估八种不确定性量化方法，分析不同ID-OOD标准（图像特征偏移和解剖特征偏移）下的OOD检测性能。

Result: OOD检测性能随分类任务显著变化，最佳任务取决于ID-OOD标准类型。同时发现优越的OOD检测性能并不保证最优的弃权预测效果。

Conclusion: 在医学图像分析中，需要根据具体下游应用来对齐任务选择和不确定性策略，单纯追求OOD检测性能可能无法满足实际应用需求。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了OrthoLoC数据集，这是首个大规模无人机图像与正射地理数据配对的基准数据集，用于解决在资源受限环境下的高精度视觉定位问题，并提出了AdHoP改进技术提升特征匹配性能。


<details>
  <summary>Details</summary>
Motivation: 在无网络或GPS支持的受限环境下，传统基于大型图像数据库或3D模型的方法不实用。正射地理数据轻量且易获取，但现有研究很少利用这种替代范式进行视觉定位。

Method: 创建包含16,425张无人机图像的OrthoLoC数据集，涵盖德国和美国多个模态。数据集设计解耦图像检索和特征匹配，支持公平评估。提出AdHoP改进技术，可与任何特征匹配器集成。

Result: 通过全面评估分析了域偏移、数据分辨率和共视性对定位精度的影响。AdHoP技术将匹配性能提升高达95%，平移误差降低高达63%。

Conclusion: OrthoLoC数据集为无人机视觉定位提供了新的基准，证明了正射地理数据的实用性。AdHoP技术显著提升了特征匹配性能，为资源受限环境下的高精度定位提供了有效解决方案。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: SSDnet是一种零样本异常检测方法，无需训练数据，仅使用单张测试图像进行自重建来定位异常。该方法基于深度图像先验，通过patch-based训练框架和感知损失实现高效异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中缺乏训练数据的问题，开发仅依赖单张测试图像的异常检测方法，避免对大量训练样本或参考样本的依赖。

Method: 设计基于patch的训练框架，直接输入图像进行自重建（而非DIP的随机噪声映射），采用掩码、patch打乱和小高斯噪声防止恒等映射，使用基于内积相似度的感知损失。

Result: 在MVTec-AD数据集上达到0.99 AUROC和0.60 AUPRC，在fabric数据集上达到0.98 AUROC和0.67 AUPRC，优于现有最先进方法。

Conclusion: SSDnet提供了一种有效的零样本异常检测解决方案，无需外部数据、标签或参考，对噪声和缺失像素具有鲁棒性，在多个数据集上表现出色。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 该论文提出了一种针对低资源孟加拉语的视觉-语言模型，通过三损失目标（PAL+InfoNCE+OT）改善图像描述生成中的对象定位问题，在Flickr30k和MSCOCO数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（特别是孟加拉语）视觉-语言模型中的对象定位问题，这些问题源于配对数据稀缺、翻译对齐破坏以及英语中心预训练忽略目标语言语义。

Method: 使用计算感知的孟加拉语描述生成流程，包括冻结的MaxViT提取视觉特征、孟加拉语原生mBART-50解码器，以及轻量级模态桥接。核心创新是三损失目标：Patch-Alignment Loss（PAL）对齐真实和合成补丁描述符，InfoNCE强制全局真实-合成分离，基于Sinkhorn的最优传输确保平衡的细粒度补丁对应。

Result: 在Flickr30k-1k上达到BLEU-4 12.29、METEOR 27.98、BERTScore-F1 71.20；在MSCOCO-1k上达到BLEU-4 12.00、METEOR 28.14、BERTScore-F1 75.40，优于强CE基线，并将真实-合成质心差距缩小41%。

Conclusion: PAL+InfoNCE+OT的协同作用有效改善了对象定位，减少了虚假匹配，为低资源语言的视觉-语言建模提供了有效解决方案。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一个轻量化的鸟瞰图框架，通过知识蒸馏将大型规划导向模型UniAD的能力压缩到小型实时学生模型中，支持完整的自动驾驶堆栈功能，参数减少78%，运行速度提升5倍。


<details>
  <summary>Details</summary>
Motivation: 解决现有高效相机基线模型功能不完整的问题，将大规模多模态感知规划模型的智能压缩到资源受限的部署环境中，实现实时自动驾驶。

Method: 采用模型无关的多阶段蒸馏策略，结合特征级、输出级和自适应区域感知监督，将高容量多模态知识转移到轻量级BEV表示中。

Result: 在nuScenes数据集上，TinyBEV达到39.0 mAP检测精度、1.08 minADE运动预测精度和0.32碰撞率，运行速度11 FPS，仅需相机输入。

Conclusion: 研究表明完整的驾驶智能可以在资源受限环境中保留，弥合了大规模多模态感知规划模型与部署就绪实时自动驾驶之间的差距。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动模糊标注策略，将球标注在模糊条纹的中心而非前缘，并开发了BlurBall模型联合估计球位置和运动模糊属性，显著提升了球类检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有标注方法将球标记在模糊条纹的前缘，存在不对称性且忽略了与速度相关的运动线索，这限制了检测系统的性能，特别是在球拍运动中球常呈现为条纹而非清晰点。

Method: 提出中心标注策略，将球置于模糊条纹中心并显式标注模糊属性；开发BlurBall模型，采用注意力机制（如Squeeze-and-Excitation）处理多帧输入，联合估计球位置和运动模糊属性。

Result: 新标注方法在各种模型上一致提升检测性能；BlurBall模型在球检测任务上达到最先进水平，同时改善了轨迹预测的可靠性。

Conclusion: 利用运动模糊信息不仅能提高检测精度，还能实现更可靠的轨迹预测，有益于实时体育分析应用。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频目标检测方法MVP，通过在关键帧上运行OWLv2检测器，利用压缩域运动向量将检测结果传播到中间帧，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 在视频中逐帧运行大型开放词汇检测器虽然准确但计算成本高昂，需要一种高效的方法来减少检测器调用次数。

Method: 使用固定间隔的关键帧策略，仅在关键帧上调用OWLv2检测器，然后利用压缩域运动向量进行3x3网格聚合，结合面积增长检查和可选的单类别切换来传播检测结果。

Result: 在ILSVRC2015-VID数据集上达到mAP@0.5=0.609和mAP@[0.5:0.95]=0.316，在宽松IoU阈值下接近逐帧检测性能，且优于基于跟踪器的传播方法。

Conclusion: 压缩域传播是一种实用的方法，可以在保持强大零样本覆盖能力的同时显著减少检测器调用次数。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本文研究HDR光照估计方法的颜色鲁棒性，发现使用预训练的白平衡网络预处理输入图像可以显著提高现有模型的颜色准确性，且无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 高动态范围光照估计方法在增强现实应用中很重要，但现有方法往往忽视颜色鲁棒性这一关键因素。本文旨在探索简单适应技术是否能提升现有模型的颜色准确性。

Method: 使用包含多样光照颜色的新HDR数据集，系统评估多种适应策略，重点测试预训练白平衡网络预处理输入图像的效果。

Result: 白平衡网络预处理方法在所有测试场景中都优于其他策略，显著提高了颜色鲁棒性。该发现适用于三种最先进的光照估计方法。

Conclusion: 简单的白平衡预处理可以有效提升现有光照估计模型的颜色准确性，无需模型重新训练，具有很好的通用性。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出一种基于视觉语言模型和多模态大语言模型的免训练支票字段检测框架，实现零样本检测支票关键字段，解决传统方法对大量标注数据的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 支票在金融系统中使用广泛但易受欺诈，传统检测方法需要大量标注数据，而由于隐私和专有性问题，这类数据稀缺。

Method: 结合视觉语言模型(VLM)和多模态大语言模型(MLLM)，开发免训练框架进行支票字段的零样本检测，无需预先训练。

Result: 在包含110张不同格式支票的手工标注数据集上验证，表现出强大的性能和泛化能力。

Conclusion: 该框架不仅能有效检测支票字段，还可作为生成高质量标注数据的引导机制，为开发专门的实时目标检测模型奠定基础。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 本文评估了视觉语言模型在图表理解中的表现，发现现有模型在图表失真或遮挡情况下性能显著下降，容易出现幻觉现象。作者提出了CHART NOISe数据集来测试模型在噪声和遮挡条件下的鲁棒性，并提出了基线缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准假设图表干净且查询基于事实，但现实世界图表常包含失真，需要超越简单匹配的推理能力。当前模型在退化设置下表现不佳且过度自信。

Method: 评估ChatGPT 4o、Claude Sonnet 4和Gemini 2.5 Pro在图表损坏和遮挡条件下的表现，引入CHART NOISe数据集（包含图表损坏、遮挡和考试风格多选题），采用提示反向不一致性测试方法。

Result: 模型在损坏或遮挡条件下性能急剧下降，幻觉现象（如数值捏造、趋势误读、实体混淆）更加频繁，模型在退化设置下仍过度自信。

Conclusion: 研究揭示了图表推理中的系统性漏洞，建立了首个统一损坏、遮挡和反向不一致性的数据集，提出了质量过滤和遮挡检测等基线缓解策略，为提升图表理解的鲁棒性和可靠性建立了严格测试平台。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于神经表示的4D-MRI重建框架，通过两个协同网络（空间解剖网络和时序运动网络）实现连续呼吸运动建模，相比传统离散排序方法显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统4D-MRI重建方法依赖相位分箱或独立模板扫描，难以捕捉时间变异性、工作流程复杂且计算负担重，需要更高效的连续运动建模方法。

Method: 使用神经表示框架，将呼吸运动建模为由1D替代信号引导的平滑连续变形。包含空间解剖网络（SAN）编码连续3D解剖表示，和时序运动网络（TMN）生成时间一致的变形场。

Result: 在19名志愿者的自由呼吸数据集上验证，方法能准确捕捉规则和不规则呼吸模式，保持血管和支气管连续性。处理时间从传统方法的5小时减少到15分钟训练时间，每个3D体积推断时间小于1秒。

Conclusion: 该框架能够准确重建任意呼吸状态下的3D图像，性能优于传统方法，在4D放射治疗规划和实时自适应治疗中具有强大应用潜力。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 该研究评估了五种基于卡尔曼滤波的跟踪方法在快速移动微小物体（如壁球）跟踪中的性能，发现DeepOCSORT在跟踪精度上表现最佳，但所有方法都存在显著的跟踪漂移问题，误差比标准物体跟踪基准高3-4倍。


<details>
  <summary>Details</summary>
Motivation: 快速移动微小物体的不可预测运动模式和小视觉标记使得精确跟踪成为计算机视觉中的挑战性问题，特别是在体育机器人应用中，轻量级准确的跟踪系统可以改善机器人感知和规划能力。

Method: 使用包含10,000个标注壁球帧的自定义数据集，评估了五种最先进的基于卡尔曼滤波的跟踪方法（OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT），重点分析推理速度和每图像更新频率对跟踪精度的影响。

Result: DeepOCSORT实现了最低的跟踪误差（平均ADE为31.15像素），而ByteTrack处理速度最快（平均推理时间26.6ms）。所有卡尔曼滤波跟踪器都表现出显著的跟踪漂移，空间误差范围为3-11cm。

Conclusion: 当前跟踪方法在处理快速移动微小物体的不可预测运动模式方面存在根本性限制，需要专门的方法论来改进，误差率比标准物体跟踪基准高3-4倍。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一个基于运动向量的自适应裁剪模块，用于压缩域视频动作识别，无需训练即可提升准确率或降低计算量


<details>
  <summary>Details</summary>
Motivation: 为了解决视频动作识别中计算效率低下的问题，利用H.264视频中已有的运动向量信息来定位运动密集区域

Method: 采用轻量级流水线，包括去噪与合并(DM)、蒙特卡洛采样(MCS)和基于运动密度子矩阵搜索的自适应裁剪(AC)

Result: 在UCF101数据集上，MoCrop在保持相同FLOPs时提升Top-1准确率3.5%，或在减少26.5% FLOPs时提升2.4%准确率；在CoViAR上达到89.2%准确率

Conclusion: MoCrop具有强通用性，适用于多种骨干网络，为压缩域实时部署提供了实用解决方案

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为CAFC-SE的基于码本的自适应特征压缩框架，通过向量量化将连续视觉特征映射为离散索引，在低比特率条件下保持更多信息性视觉模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低比特率条件下性能较差，因为它们保留了过多冗余细节或学习过度集中的符号分布，需要一种更有效的边缘-云系统图像编码方法。

Method: 使用向量量化(VQ)将连续视觉特征映射到码本中的离散索引，选择性传输到云端，通过投影到最近的视觉基元来保留信息性视觉模式。

Result: 大量实验证明该方法在速率和准确性方面具有优越性，特别是在低比特率条件下表现更稳定。

Conclusion: CAFC-SE框架通过码本自适应特征压缩和语义增强，有效解决了低比特率条件下的特征压缩问题，提升了边缘-云系统的分析性能。

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet是一种超轻量级多核U形CNN架构，专为医学图像分割设计，在保持极低计算资源（0.316M参数，0.314G FLOPs）的同时，在六个医学图像基准测试中超越了现有最先进方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 为了解决医学图像分割在资源受限环境（如床旁设备）中的实时高保真诊断需求，需要开发计算效率高且性能优越的轻量级网络架构。

Method: 设计了多核深度卷积块（MKDC）来处理多分辨率空间关系，并采用通道、空间和分组门控注意力机制来突出图像显著特征。

Result: MK-UNet在DICE分数上超越TransUNet，参数减少333倍，FLOPs减少123倍；相比UNeXt，DICE分数提升高达6.7%，参数减少4.7倍；同时优于其他轻量级网络如MedT、CMUNeXt等。

Conclusion: MK-UNet在计算资源和分割性能之间实现了突破性平衡，为资源受限环境下的实时医学诊断提供了无与伦比的解决方案。

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat是一种新颖的可变形手术导航方法，通过将术中3D重建与术前CT数据耦合，在手术视频和体积患者数据之间建立桥梁。该方法将3D高斯分布与CT网格绑定，通过光度监督联合优化高斯参数和网格变形。


<details>
  <summary>Details</summary>
Motivation: 解决手术导航中手术视频与术前CT数据之间的差距问题，实现更准确的可变形手术导航。

Method: 将3D高斯分布绑定到CT网格上，通过参数化每个高斯分布相对于其父网格三角形的位置，强制高斯分布与网格对齐，并通过光度监督联合优化高斯参数和网格变形。

Result: 在猪内脏手术和人类肝脏合成数据上验证了方法的有效性，能够在单目RGB数据上实现合理的术前CT变形。

Conclusion: BridgeSplat方法成功地将术中3D重建与术前CT数据相结合，为可变形手术导航提供了有效的解决方案。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 本文提出了一种名为DGLE的扩散引导标签增强框架，用于解决源自由域适应中伪标签噪声问题。该方法从少量高质量伪标签出发，利用扩散模型传播生成完整的高质量伪标签集，避免直接优化噪声标签的困难。


<details>
  <summary>Details</summary>
Motivation: 源自由域适应中，自训练方法需要高质量伪标签，但现有方法直接优化整个噪声伪标签集效果有限。需要一种能从少量高质量标签出发，逐步生成完整高质量标签集的方法。

Method: 1) 基于置信度过滤和超分辨率增强的伪标签融合方法获取高质量初始种子标签；2) 利用扩散模型对不规则分布的种子标签进行传播，生成完整高质量伪标签。

Result: DGLE框架有效避免了直接优化完整伪标签集的困难，显著提升了伪标签质量，从而增强了模型在目标域的性能。

Conclusion: 提出的扩散引导标签增强方法为源自由域适应中的伪标签优化问题提供了有效解决方案，通过从少量高质量标签出发，利用扩散模型生成完整高质量标签集，显著提升了语义分割性能。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种在双曲空间中实现粗到细少样本类增量学习的方法，通过双曲嵌入、对比学习和最大熵分布来提升分层数据的表示能力和少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 双曲空间相比欧几里得空间在表示分层数据方面具有优势，而现有的粗到细少样本类增量学习方法主要基于欧几里得空间，未能充分利用双曲空间的表示能力。

Method: 使用庞加莱球模型将特征提取器嵌入双曲空间，引入双曲对比损失和双曲全连接层，并采用双曲空间中的最大熵分布进行特征增强以缓解少样本过拟合问题。

Result: 在C2FSCIL基准测试中，该方法有效提升了粗类和细类分类的准确率。

Conclusion: 双曲空间嵌入为粗到细少样本类增量学习提供了更有效的表示方法，特别是在处理分层数据和少样本场景时表现出显著优势。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种几何感知的两阶段框架GeoRemover，用于智能图像编辑中的物体移除，能够同时移除目标物体及其因果视觉伪影（如阴影和反射），解决了现有方法要么无法移除未明确掩码的因果效应，要么缺乏可控性可能过度擦除其他物体的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在物体移除时存在局限性：基于严格掩码对齐的方法无法移除目标物体的因果视觉伪影（如阴影、反射），而基于松散掩码对齐的方法缺乏可控性且可能过度擦除其他物体。这些限制源于忽视了物体几何存在与其视觉效应之间的因果关系。

Method: 提出几何感知的两阶段框架：1）几何移除阶段：使用严格掩码对齐监督直接从几何（如深度）中移除物体，实现具有强几何约束的结构感知编辑；2）外观渲染阶段：基于更新后的几何条件渲染逼真的RGB图像，其中因果视觉效应作为修改后3D几何的结果被隐式考虑。还引入了基于正负样本对的偏好驱动目标来指导几何移除阶段的学习。

Result: 在两个流行基准测试上的广泛实验表明，该方法在移除物体及其相关伪影方面达到了最先进的性能。

Conclusion: GeoRemover通过将物体移除解耦为几何移除和外观渲染两个阶段，有效解决了现有方法在移除因果视觉伪影方面的局限性，实现了更智能和可控的图像编辑。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 本文提出了SEGA方法，首次解决NR-IQA模型攻击中低迁移性的挑战，通过在黑盒场景下使用高斯平滑和梯度集成技术来提高对抗攻击的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有的NR-IQA模型白盒攻击方法在更现实的黑盒场景中迁移性较差，因为目标模型不可访问，这限制了攻击方法在实际应用中的有效性。

Method: SEGA方法通过应用高斯平滑到源模型并集成它们的平滑梯度来近似目标模型的梯度，同时使用专门设计的扰动过滤掩码来确保对抗扰动的不可感知性。

Result: 在CLIVE数据集上的实验结果表明，SEGA具有优越的迁移性，能够成功实现对NR-IQA模型的基于迁移的黑盒攻击。

Conclusion: SEGA方法有效解决了NR-IQA模型黑盒攻击中的迁移性问题，为揭示模型脆弱性和指导鲁棒系统设计提供了有价值的工具。

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: 本文提出HadaSmileNet框架，通过Hadamard乘法融合transformer表示和D-Marker特征，在微笑情感识别任务上实现高效且高性能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习方法存在计算效率低、需要辅助任务监督和复杂损失平衡的问题，需要更高效的特征融合方法。

Method: 使用参数自由的Hadamard乘法直接融合transformer表示和生理学基础的D-Marker特征，系统评估了15种融合策略。

Result: 在四个基准数据集上达到SOTA性能：UvA-NEMO(88.7%)、MMI(99.7%)、SPOS(98.5%)、BBC(100%)，参数减少26%，训练更简化。

Conclusion: HadaSmileNet框架通过直接域知识集成增强了判别能力，特别适合需要实时情感计算能力的多媒体数据挖掘应用。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机和3D高斯泼溅的联合动态人体与静态场景重建框架，通过事件引导的损失函数解决快速运动下的模糊问题。


<details>
  <summary>Details</summary>
Motivation: 从单目视频中重建动态人体和静态场景存在困难，特别是在快速运动时RGB帧会出现运动模糊。事件相机具有微秒级时间分辨率的优势，更适合动态人体重建。

Method: 使用统一的3D高斯集合，其中可学习的语义属性区分人体和场景高斯；人体高斯进行变形动画，场景高斯保持静态；提出事件引导损失函数，匹配连续渲染之间的亮度变化与事件流。

Result: 在ZJU-MoCap-Blur和MMHPSD-Blur两个基准数据集上实现了最先进的人体-场景重建效果，在PSNR/SSIM指标上显著优于基线方法，LPIPS指标降低，特别对高速运动主体效果更好。

Conclusion: 该方法无需外部人体掩码，简化了高斯集合管理，通过事件引导有效解决了快速运动下的模糊问题，为动态人体与静态场景的联合重建提供了有效解决方案。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T是一个实时威胁监控框架，通过结构化语义元组、在线事件去重和基于大语言模型的推理机制，同时实现高性能威胁检测和决策可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足实时性能和决策可解释性的要求，需要一种统一框架来解决这一挑战。

Method: 1）将视频帧解构为结构化的人-物-交互-地点语义元组；2）提出高效的在线事件去重和更新机制；3）使用思维链策略微调大语言模型进行透明推理。

Result: 在XD-Violence和UCF-Crime等基准数据集上的实验表明，Live-E2T在威胁检测精度、实时效率和可解释性方面显著优于最先进方法。

Conclusion: Live-E2T成功统一了实时性能和可解释性目标，为实时威胁监控提供了有效的解决方案。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 本文提出了一个名为PhotoCritique的新数据集、PhotoEye模型和PhotoBench基准测试，旨在提升多模态大语言模型在美学视觉理解方面的能力，特别是针对摄影专业知识的深度分析。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在美学视觉理解方面存在局限，主要关注一般性视觉理解（如物体检测）而缺乏专业的摄影美学知识（如色彩、构图、后期处理等），难以满足真实场景中对专业摄影分析的需求。

Method: 1. 构建PhotoCritique数据集：基于专业摄影师和爱好者的广泛讨论，具有大规模、专业性和多样性特点；
2. 提出PhotoEye模型：采用语言引导的多视角视觉融合机制，从多个角度理解图像美学；
3. 建立PhotoBench基准测试：全面专业的视觉美学理解评估标准。

Result: 在现有基准测试和PhotoBench上，PhotoEye模型相比现有模型展现出明显优势，证明了其在美学视觉理解方面的有效性。

Conclusion: 通过专业数据集、创新模型和全面基准测试的结合，本文为多模态大语言模型的美学视觉理解能力提供了系统性的提升方案，填补了专业摄影美学分析的技术空白。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 基于XMem模型的实时MRI引导放疗肿瘤分割框架，用于TrackRAD2025挑战赛，能够对长序列cine-MRI进行肿瘤分割和运动跟踪。


<details>
  <summary>Details</summary>
Motivation: 提高MRI引导放疗中肿瘤跟踪的精度，这对于提升癌症治疗的准确性和安全性至关重要。

Method: 利用XMem（内存增强架构）模型，集成内存机制来实时跟踪肿瘤运动，即使在标注数据有限的情况下也能实现高分割精度。

Result: 由于详细实验记录丢失，无法报告精确的定量结果，但从开发过程中的初步印象来看，基于XMem的框架表现出合理的分割性能，并满足临床实时要求。

Conclusion: 该工作有助于改善MRI引导放疗期间的肿瘤跟踪精度，为癌症治疗提供了更准确和安全的解决方案。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SSCM模型解决多对比度MRI超分辨率问题，通过动态空间扭曲、语义感知令牌聚合和空间频率融合模块，在保持空间语义一致性的同时提升图像质量


<details>
  <summary>Details</summary>
Motivation: 传统方法在空间语义一致性建模不足且未能充分利用频域信息，导致细粒度对齐差和高频细节恢复不足

Method: SSCM模型包含三个核心模块：动态空间扭曲模块用于跨对比度空间对齐，语义感知令牌聚合块用于长程语义一致性，空间频率融合块用于精细结构恢复

Result: 在公开和私有数据集上的实验表明，SSCM以更少的参数实现了最先进的性能，同时确保空间和语义一致的重建

Conclusion: SSCM模型有效解决了多对比度MRI超分辨率中的空间语义一致性问题，显著提升了图像质量和重建精度

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出OraPO方法和FactS奖励机制，在有限计算资源下实现高效的放射学报告生成，显著减少训练数据需求并达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决现有放射学报告生成方法对大规模数据和计算资源的高度依赖问题，在预算受限条件下实现高效训练

Method: 结合Oracle-educated GRPO (OraPO)和FactScore-based奖励机制(FactS)，通过单阶段强化学习训练，将失败的探索转化为偏好监督，基于临床事实提取和验证提供密集可解释的奖励

Result: 在CheXpert Plus数据集上达到0.341 F1分数的新SOTA性能，训练数据减少2-3个数量级，使用小型基础视觉语言模型和普通硬件

Conclusion: OraPO和FactS构建了一个紧凑而强大的框架，显著提高了临床挑战性病例的学习效率，为资源受限环境下的放射学报告生成提供了有效解决方案

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一个无需训练的参考式多风格融合框架，能够在扩散模型中实现可控的多参考风格融合，解决了现有方法只能处理单风格图像和缺乏平衡多种风格影响的机制问题。


<details>
  <summary>Details</summary>
Motivation: 现有参考式方法存在两个主要限制：(a)只能接受一个风格图像，无法实现混合美学和扩展到更多风格；(b)缺乏平衡多种风格影响的机制。AMSF旨在解决这些问题，实现更灵活的多风格生成。

Method: 通过语义标记分解模块编码所有风格图像和文本提示，自适应注入到冻结扩散模型的每个交叉注意力层中。相似性感知重加权模块在每个去噪步骤重新校准对每个风格组件的注意力分配。

Result: 定性和定量评估显示，AMSF产生的多风格融合结果始终优于最先进方法，其融合设计可无缝扩展到两种或更多风格。

Conclusion: AMSF为扩散模型中的表达性多风格生成提供了实用步骤，实现了无需微调或外部适配器的平衡且用户可控的风格融合。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: MLF-4DRCNet是一个用于3D目标检测的两阶段框架，通过4D雷达和相机图像的多级融合来解决雷达点云稀疏和噪声问题，在VoD和TJ4DRadSet数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达虽然成本效益高且鲁棒性强，但其点云存在显著稀疏性和噪声，限制了在3D目标检测中的独立应用。现有的雷达-相机融合方法大多采用为LiDAR设计的BEV融合范式，忽略了雷达的固有缺陷。

Method: 提出MLF-4DRCNet框架，包含三个关键模块：增强雷达点编码器（ERPE）模块通过三重注意力体素特征编码器将雷达点云密度化并编码为体素；分层场景融合池化（HSFP）模块使用可变形注意力动态整合多尺度体素特征与2D图像特征；提案级融合增强（PLFE）模块通过融合图像特征来优化区域提案。

Result: 在View-of-Delft（VoD）和TJ4DRadSet数据集上的实验结果表明，MLF-4DRCNet实现了最先进的性能，在VoD数据集上达到了与基于LiDAR的模型相当的性能。

Conclusion: MLF-4DRCNet通过点级、场景级和提案级的多模态信息融合，实现了全面的特征表示，有效解决了4D雷达点云稀疏和噪声问题，为自动驾驶中的3D目标检测提供了有效的解决方案。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出Prompt-Guided Dual Latent Steering (PDLS)框架，通过双潜在流解决扩散模型图像反演中的语义漂移问题，在保持结构完整性的同时提升语义准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于单潜在向量的图像反演方法难以平衡结构保真度和语义准确性，导致重建图像出现语义漂移问题，如细节模糊或属性错误。

Method: 基于Rectified Flow模型构建训练免费的PDLS框架，将反演过程分解为结构路径和语义路径，通过线性二次调节器(LQR)作为最优控制器动态引导生成轨迹。

Result: 在FFHQ-1K和ImageNet-1K数据集上的多种反演任务（高斯去模糊、运动去模糊、超分辨率和自由形式修复）实验表明，PDLS比单潜在基线方法产生更忠实于原始图像且语义对齐更好的重建结果。

Conclusion: PDLS通过双潜在流引导和最优控制方法，有效解决了扩散模型图像反演中的语义漂移问题，无需昂贵的逐图像优化即可实现高质量重建。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 开发了Prima，首个用于神经影像的视觉语言模型，在52种神经疾病诊断中平均AUC达到92.0%，优于现有AI模型，能够提供可解释的鉴别诊断和临床推荐。


<details>
  <summary>Details</summary>
Motivation: 全球MRI需求持续增长给医疗系统带来压力，延长了报告时间并加剧医生倦怠，这些问题在资源匮乏地区尤为严重。

Method: 利用大型学术医疗系统作为数据引擎，训练基于22万例MRI研究的视觉语言模型Prima，采用分层视觉架构提供通用可迁移的MRI特征。

Result: 在包含3万例MRI研究的1年系统测试中，Prima在主要神经疾病诊断中表现优异，平均AUC达92.0%，且在不同患者群体和MRI系统中保持算法公平性。

Conclusion: Prima展示了医疗系统规模视觉语言模型的变革潜力，有助于缓解医疗系统偏见，推进AI驱动的医疗保健发展。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的统一模型推理框架Understanding-in-Generation (UiG)，通过将理解能力融入生成过程来提升文本到图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推理方法将理解和生成过程分离，限制了统一模型在解决生成能力不足问题上的指导能力。

Method: UiG框架通过"图像编辑"作为桥梁，在推理过程中融入强大的理解能力来指导生成。首先验证生成图像并将模型理解融入编辑指令，然后逐步增强生成图像。

Result: UiG在文本到图像生成任务上显著优于现有方法，在TIIF基准测试的长提示设置中获得了3.92%的性能提升。

Conclusion: UiG框架成功地将理解能力融入生成过程，有效缓解了统一模型生成能力的局限性，为文本到图像生成提供了新的推理范式。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 这篇论文提出了一个用于内窥镜图像的深度估计综合基准测试，并发布了新的合成数据集EndoSynth，通过微调深度基础模型显著提升了在真实内窥镜数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像深度估计领域缺乏稳健的基准测试和高质量数据集，限制了该技术在临床场景中的应用和发展。

Method: 1) 建立综合基准测试评估现有最先进的深度估计模型在真实内窥镜图像上的表现；2) 创建并发布包含真实内窥镜手术器械的合成数据集EndoSynth，提供真实深度和分割掩码；3) 使用合成数据集微调深度基础模型。

Result: 实验表明，使用EndoSynth合成数据集微调的深度基础模型在大多数未见过的真实数据上准确率显著提升。

Conclusion: 该工作通过提供基准测试和合成数据集，推动了内窥镜图像深度估计领域的发展，为未来研究提供了重要资源。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出LEAF-Mamba模型，通过局部强调状态空间模块和自适应融合模块，在RGB-D显著目标检测任务中实现性能与计算效率的平衡


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D显著目标检测方法要么受限于CNN的局部感受野，要么受限于Vision Transformer的二次复杂度，难以平衡性能与计算效率。状态空间模型(SSM)虽能建模长距离依赖，但直接应用于RGB-D SOD会导致局部语义不足和跨模态融合不充分

Method: 提出LEAF-Mamba模型，包含两个核心组件：1)局部强调状态空间模块(LE-SSM)捕获多尺度局部依赖；2)基于SSM的自适应融合模块(AFM)实现互补的跨模态交互和可靠的跨模态集成

Result: 在16个最先进的RGB-D SOD方法中，LEAF-Mamba在效能和效率方面均表现最优。在RGB-T SOD任务上也表现出色，证明其强大的泛化能力

Conclusion: LEAF-Mamba模型成功解决了RGB-D SOD中局部语义不足和跨模态融合不充分的问题，在保持线性复杂度的同时实现了优异的性能

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出了一种结合窗口多头注意力机制和空间注意力机制的轻量级食品图像分类算法，在降低计算复杂度的同时保持高分类精度


<details>
  <summary>Details</summary>
Motivation: 食品行业对生产质量和效率要求不断提高，但现有Vision Transformer模型参数多、计算复杂度高，难以在资源受限环境中部署

Method: 集成窗口多头注意力机制(WMHAM)和空间注意力机制(SAM)，WMHAM通过窗口划分降低计算成本，SAM自适应强调关键空间区域

Result: 在Food-101和Vireo Food-172数据集上分别达到95.24%和94.33%的准确率，同时显著减少参数和FLOPs

Conclusion: 该方法在计算效率和分类性能之间取得了有效平衡，适合在资源受限环境中部署

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA是一个用于遥感图像开放集土地覆盖分析的三阶段框架，能够实现无标注的发现、分割和描述，结合像素级精度和高层语义理解


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像中开放集土地覆盖分析的需求，需要同时实现细粒度空间定位和语义开放分类，既要检测和分割无类别监督的新对象，又要通过多模态推理为其分配可解释的语义标签

Method: 三阶段框架：1）使用可提示的微调分割模型（SAM）进行精确发现和掩码提取；2）通过两阶段微调的多模态大语言模型（MLLM）进行语义归因和上下文描述；3）使用LLM作为评判者并结合人工评分进行MLLM评估

Result: 该框架提供了可扩展和可解释的解决方案，支持在多样化卫星图像上进行鲁棒评估，无需人工标注，在动态土地覆盖监测方面显示出强大潜力

Conclusion: OSDA框架为自动化制图更新和大规模地球观测分析提供了有效的解决方案，通过结合像素级精度和高层语义理解，解决了开放世界遥感解释中的关键挑战

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了LifeCLEF 2021植物识别挑战赛，旨在评估如何利用植物标本馆数据改进生物多样性丰富但数据匮乏地区的植物自动识别。


<details>
  <summary>Details</summary>
Motivation: 当前植物自动识别主要基于北美和西欧的数据，而生物多样性最丰富的热带地区数据稀缺。但植物标本馆中保存了大量热带植物标本，可用于弥补这一数据缺口。

Method: 采用跨领域分类任务，训练集包含数十万份植物标本馆标本和数千张野外照片，测试集仅包含野外照片。数据还包括5种形态和功能性状值。

Result: 挑战赛基于圭亚那地盾地区的约1000种植物进行评估，该地区是全球植物多样性最高的地区之一。

Conclusion: 通过整合植物标本馆数据和野外照片，可以显著改善数据匮乏地区的植物自动识别性能。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出AGSwap方法解决文本到图像生成中跨类别对象融合的问题，并构建了COF数据集进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨类别对象融合时存在偏差、视觉混乱和语义不一致的问题，且缺乏全面的基准数据集。

Method: AGSwap方法包含两个关键组件：组间嵌入交换和自适应组更新，通过特征操作和动态优化机制实现语义属性的融合。

Result: 实验表明AGSwap在简单和复杂提示下均优于现有最先进的组合T2I方法，包括GPT-Image-1。

Conclusion: AGSwap是一种简单而高效的方法，结合COF数据集为跨类别对象融合任务提供了有效的解决方案。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了LifeCLEF 2019植物识别挑战赛，旨在评估在数据稀缺地区（如圭亚那地盾和北亚马逊雨林）的自动化植物识别系统性能，并与热带植物专家进行比较。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习植物识别主要针对少数常见物种，而全球有近369,000种植物，许多物种缺乏数据。该挑战赛旨在解决数据稀缺地区的植物识别问题。

Method: 基于10,000种主要来自圭亚那地盾和北亚马逊雨林区域的植物数据集，组织植物识别挑战赛，评估不同研究团队的自动化识别系统，并与专家识别结果进行比较。

Result: 挑战赛评估了各参与研究团队的植物识别系统性能，分析了在数据稀缺环境下的识别效果。

Conclusion: 该挑战赛为数据稀缺地区的植物识别研究提供了重要资源和评估基准，推动了自动化植物识别技术在生物多样性保护中的应用。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV是一个无需训练的零样本开放词汇遥感视觉定位框架，利用冻结的基础模型实现开放世界场景下的目标定位


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感视觉定位方法受限于封闭词汇集、依赖高质量数据集和耗时微调的问题，探索冻结通用基础模型在零样本开放词汇场景下的潜力

Method: 三阶段框架：1) 使用视觉语言模型获取跨注意力图；2) 利用扩散模型补充结构信息；3) 通过注意力进化模块净化分割掩码

Result: 在广泛实验中，该框架持续优于现有的弱监督和零样本方法

Conclusion: RSVG-ZeroOV提供了一个高效可扩展的解决方案，无需繁琐的任务特定训练

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种属性提示组合（APC）框架，通过利用文本语义来联合增强目标重识别（ReID）的判别性和泛化能力。该框架包含属性提示生成器和快慢训练策略，在传统和领域泛化ReID数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型大多局限于单域或跨域场景，单域模型容易过拟合到特定域特征，而跨域模型通常依赖多样化归一化策略，可能无意中抑制身份特定的判别线索。需要一种既能增强判别性又能提升泛化能力的方法。

Method: 提出属性提示组合（APC）框架：1）属性提示生成器（APG），包含语义属性字典（SAD）和提示组合模块（PCM）；2）快慢训练策略（FSTS），包含快速更新流（FUS）和慢速更新流（SUS），平衡ReID特定判别性和通用表示学习。

Result: 在传统和领域泛化ReID数据集上的大量实验表明，该框架超越了最先进的方法，在判别性和泛化性方面均表现出优越性能。

Conclusion: APC框架通过文本语义的有效利用，成功解决了ReID任务中判别性与泛化性的平衡问题，为实际应用提供了更实用的解决方案。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: 本文提出OTCCLIP框架，使用最优传输理论重建图像-文本对，通过细粒度特征对齐来防御CLIP模型的数据中毒和后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法仅依赖图像和文本的全局表示，忽略了细粒度特征，可能导致错误的图像-文本配对，损害CLIP预训练效果。

Method: 提出基于最优传输的细粒度视觉和文本特征集距离度量，重新分配文本描述，并采用最优传输目标函数促进模态间和模态内的细粒度对齐。

Result: OTCCLIP成功降低了中毒攻击的成功率，相比之前方法显著提高了CLIP在中毒数据集上的零样本和线性探测性能。

Conclusion: 基于最优传输的细粒度特征对齐方法能有效防御CLIP模型的数据中毒攻击，并提升模型性能。

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 提出了LFI框架，通过显式建模视觉理解作为交互过程，解决视觉基础模型从视觉语言模型知识转移的局限性，在多个基准测试中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前视觉基础模型在从视觉语言模型转移知识时存在根本限制，主要采用结果导向范式而忽略了底层的交互过程，这种表示差异阻碍了有效的知识转移并限制了跨不同视觉任务的泛化能力。

Method: 提出LFI框架，核心是两个技术创新：交互查询（维护跨网络层的持久关系结构）和基于交互的监督（源自视觉语言模型的跨模态注意力机制）。

Result: 在多个基准测试中取得一致改进：TinyImageNet分类提升3.3和1.6mAP，COCO检测/分割提升2.4AP，在跨域设置中表现优异（PACS和VLCS零样本提升2.4和9.3），人类评估显示语义一致性指标比结果导向方法高2.7倍。

Conclusion: LFI框架通过建模交互过程实现了更忠实和高效的知识转移，具有最小的参数开销和更快的收敛速度，在认知对齐方面表现优异。

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 提出HyPSAM模型，利用SAM的零样本泛化能力进行RGB-热成像显著目标检测，通过动态融合网络和即插即用优化网络实现多模态特征融合和精确定位


<details>
  <summary>Details</summary>
Motivation: 解决RGB-热成像显著目标检测中特征融合不充分和数据稀缺的问题，利用SAM的强大分割能力提升检测性能

Method: 1. 动态融合网络(DFNet)生成高质量初始显著图作为视觉提示；2. 即插即用优化网络(P2RNet)使用混合提示（文本、掩码、边界框）指导SAM优化显著图

Result: 在三个公开数据集上实现最先进性能，具有显著泛化能力，可与其他RGB-T SOD方法无缝集成

Conclusion: HyPSAM展示了提示工程在RGB-T SOD领域的潜力，为多模态显著目标检测提供了有效解决方案

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE是一种多模态交叉注意力自编码器，通过整合文本先验、多视图图像的单目深度图和LiDAR点云来提高点云去噪和重建的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在自动驾驶和机器人感知中至关重要，但原始点云容易受到噪声、遮挡和对抗性破坏的影响。现有自编码器在真实世界的挑战性条件下性能下降。

Method: 提出TriFusion-AE模型，通过跨注意力机制对齐文本的语义线索、图像的几何特征和LiDAR的空间结构，学习对随机噪声和对抗性扰动具有鲁棒性的表示。

Result: 在nuScenes-mini数据集上评估显示，虽然轻微扰动下增益有限，但在强对抗攻击和重噪声条件下，模型显著优于基于CNN的自编码器。

Conclusion: TriFusion-AE提供了一个模型无关的多模态融合框架，可与任何基于CNN的点云自编码器无缝集成进行联合表示学习。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 提出COLT方法，增强开源视频LLMs的持续工具使用能力，解决现有方法在动态工具环境中的泛化问题


<details>
  <summary>Details</summary>
Motivation: 现有视频LLMs工具使用方法假设工具库固定，难以适应现实世界中工具数据持续演化和流式输入的环境

Method: COLT方法包含可学习的工具码本作为工具特定记忆系统，根据用户指令与工具特征的相似度动态选择相关工具，并收集VideoToolBench数据集进行指令调优

Result: 在视频LLM基准测试和工具使用特定的VideoToolBench数据集上展示了最先进的性能

Conclusion: COLT方法成功实现了视频LLMs在连续工具流中的自动工具使用能力，避免了灾难性遗忘问题

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS是一个无需训练的方法，利用现有扩散模型增强稀疏视角3D高斯溅射重建，通过改进的蒸馏方法和自适应渐进增强方案解决多视角一致性问题


<details>
  <summary>Details</summary>
Motivation: 稀疏视角3D重建存在视觉信息不足的问题，导致明显的伪影，现有基于生成先验的方法难以保证多视角一致性

Method: 提出FixingGS方法，包括更准确的跨视角一致扩散先验蒸馏方法和自适应渐进增强方案

Result: 实验表明FixingGS在视觉质量和重建性能上优于现有最先进方法

Conclusion: FixingGS通过有效利用扩散模型能力，成功解决了稀疏视角3DGS重建中的伪影和多视角一致性问题

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: 提出了Bi-VLM方法，通过非均匀分离模型权重来解决视觉语言模型在超低比特权重精度下的效率问题，在多个基准测试中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的计算需求和内存要求很高，限制了其在硬件受限环境中的应用，需要解决超低比特权重精度下的效率问题。

Method: 提出Bi-VLM方法，基于高斯分位数非均匀分离模型权重，将权重分为异常值（显著）和多个正常值（不显著）子集，并提出显著性感知混合量化算法。

Result: 在视觉问答任务中，Bi-VLM在语言模型部分比现有技术提升3%-47%，在整个VLM上提升4%-45%，并发现量化模型中图像令牌存在90%-99%的冗余。

Conclusion: Bi-VLM方法有效解决了VLMs在超低比特精度下的效率问题，通过显著性感知量化和令牌剪枝进一步提升了模型效率。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT是一个自监督学习框架，通过多尺度向量量化在SSL管道中施加离散表示瓶颈，抑制视图特定或低效用模式，提高跨任务和领域的表示迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有SSL方法依赖复杂架构、解剖学先验或精心调优的增强，限制了可扩展性和泛化性，且容易在胸部X光等模态中出现捷径学习问题。

Method: 集成多尺度向量量化到SSL管道，施加离散表示瓶颈，约束模型学习可重复、结构感知的特征。

Result: 在分类和分割任务上表现优异，需要最小化或无需微调，在低标签情况下表现出高标签效率。

Conclusion: DiSSECT在多个公共医学影像数据集上验证了其鲁棒性和泛化性，优于现有最先进方法。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 本文提出了一种结合热成像、深度学习和车联网通信的实时检测与驾驶员预警系统，用于减少鹿车碰撞事故。系统在12000张热成像鹿图像数据集上训练，达到98.84%的平均精度，并在实地测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 美国每年发生约210万起鹿车碰撞事故，造成440人死亡、59000人受伤和100亿美元经济损失，同时导致鹿群数量下降。现有可见光摄像头在恶劣天气条件下检测效果不足60%，需要更可靠的解决方案。

Method: 系统整合热成像技术、深度学习算法和C-V2X通信技术。使用在北卡罗来纳州Mars Hill收集的12000张热成像鹿图像数据集进行训练和验证，当检测到高概率目标时通过C-V2X设备向周围车辆和路边单元广播预警信息。

Result: 系统表现优异：平均精度98.84%，精确率95.44%，召回率95.96%。实地测试显示热成像在恶劣天气条件下保持88-92%的检测准确率，而传统可见光摄像头低于60%。端到端延迟始终低于100毫秒。

Conclusion: 该研究通过热成像和联网车辆技术为减少鹿车碰撞事故建立了可行的技术路径，系统在各种天气条件下都具有鲁棒性，能够为驾驶员提供及时预警。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [70] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi框架通过将扩散模型与下游任务对齐，解决了手术数据稀缺问题，显著提升了分类和分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 手术数据标注稀缺限制了深度学习系统在计算机辅助干预中的发展，而传统扩散模型存在数据记忆化问题，导致生成样本不一致或缺乏多样性，可能损害下游任务性能。

Method: 提出Surgical Application-Aligned Diffusion (SAADi)框架，构建偏好和非偏好合成图像对，通过轻量级微调使扩散模型的图像生成过程与下游目标明确对齐。

Result: 在三个手术数据集上的实验显示，分类任务提升7-9%，分割任务提升2-10%，对代表性不足的类别改进尤为显著。迭代细化合成样本可进一步提升性能4-10%。

Conclusion: SAADi方法克服了样本退化问题，确立了任务感知对齐作为缓解数据稀缺和推进手术视觉应用的关键原则。

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [71] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出基于模型神经网络的动态PET图像去噪方法KMDS-Net，利用帧间空间相关性和帧内结构一致性建立多维稀疏模型，通过神经网络自适应优化参数，在模拟和真实数据上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 动态PET中时间帧的图像质量提升具有挑战性，特别是短帧的统计信息有限。深度学习在医学图像去噪任务中显示出广泛适用性。

Method: 建立基于核空间的多维稀疏(KMDS)模型，利用动态PET的帧间空间相关性和帧内结构一致性，然后用神经网络替代参数估计的内在形式，形成端到端的KMDS-Net。

Result: 模拟和真实数据的广泛实验结果表明，KMDS-Net在动态PET去噪方面表现出强大的性能，优于之前的基线方法。

Conclusion: 该方法可有效实现动态PET的高时间和空间分辨率，源代码已公开。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [72] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 提出一种结合光流分割标签插值和多任务学习的新框架，解决机器人辅助手术中时空标注不平衡问题，提升手术场景理解能力


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术需要精确理解视觉数据，但现有方法多为单任务，且像素级分割数据标注成本高，导致长期标注（阶段/步骤）与短期标注（器械分割/动作检测）之间存在显著时空不平衡

Method: 使用光流估计从标注关键帧传播标签到相邻未标注帧，通过标签插值丰富稀疏空间监督，结合多任务学习平衡时空信息

Result: 该方法改善了手术场景理解的准确性和效率

Conclusion: 该框架通过解决标注不平衡问题，增强了机器人辅助手术的实用性

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [73] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel是一个统一的多模态模型加速框架，通过分治策略同时加速多模态理解和生成任务，实现了2倍以上的理解加速和16-22倍的生成加速。


<details>
  <summary>Details</summary>
Motivation: 随着多模态上下文整合越来越多的交错多模态token，扩散去噪和自回归解码的迭代过程带来了显著的计算开销，需要高效的加速解决方案。

Method: 采用分治策略，使用推测解码进行下一token预测，并通过多阶段蒸馏过程加速扩散去噪。结合对抗蒸馏和人类反馈学习开发高效模型。

Result: 实现了多模态理解超过2倍加速，文本到图像生成16.67倍加速，图像编辑22倍加速，同时保持原始模型的高质量输出。开发了高效的1-NFE模型实现近实时交互。

Conclusion: Hyper-Bagel框架通过先进的加速技术实现了显著的计算效率提升，使复杂的多模态交互变得无缝和即时，具有极高的成本效益和响应性。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [74] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 本研究评估了多模态大语言模型在基督教圣像单标签分类任务中的表现，发现GPT-4o和Gemini 2.5等模型在复杂文化遗产领域的分类能力优于传统ResNet50基线模型。


<details>
  <summary>Details</summary>
Motivation: 评估通用多模态LLMs是否能够解释通常由监督分类器处理的基督教圣像图像，并测试其在文化传承领域的应用潜力。

Method: 使用三个支持Iconclass的数据集（ArtDL、ICONCLASS、Wikidata），在三种条件下测试模型：仅类别标签、带Iconclass描述、以及五样本少样本学习。

Result: Gemini-2.5 Pro和GPT-4o表现优于ResNet50基线；添加类别描述通常能提升零样本性能；少样本学习效果较差；Wikidata数据集上准确率显著下降。

Conclusion: 通用多模态LLMs能够在视觉复杂的文化遗产领域进行分类，支持将其作为数字人文工作流中的元数据管理工具。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [75] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 本文提出了一种可学习的重参数化图构建方法（LRGC）用于视觉图神经网络，解决了传统ViG模型中图构建方法不可学习和依赖超参数的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉图神经网络使用非参数化、不可学习的统计方法来构建图结构，这些方法可能无法为每个节点选择最佳邻域，且需要超参数搜索。

Method: LRGC方法在每对节点之间应用键值注意力机制，然后使用软阈值重参数化进行边选择，这使得可以使用可微的数学模型进行训练。

Result: 在ImageNet-1k基准数据集上，提出的ViG-LRGC方法在相似模型大小下优于最先进的ViG模型。

Conclusion: LRGC通过可学习的阈值参数实现了无需超参数搜索的图构建，消除了传统聚类或阈值方法引入的偏差，提高了ViG模型的性能。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [76] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 提出结构化反思方法，将错误修复路径转化为明确、可控且可训练的动作，通过Reflect-Call-Final策略优化工具调用，显著提升多轮工具调用成功率和错误恢复能力


<details>
  <summary>Details</summary>
Motivation: 当前工具增强LLM的训练方法依赖监督模仿或粗粒度强化学习，自我反思实践基于启发式提示或单向推理，在多轮交互中脆弱且易重复错误

Method: 结合DAPO和GSPO目标与针对工具使用的奖励方案，优化Reflect-Call-Final逐步策略，引入Tool-Reflection-Bench基准程序化检查结构有效性、可执行性、参数正确性和结果一致性

Result: 在BFCL v3和Tool-Reflection-Bench上的实验显示多轮工具调用成功率和错误恢复能力大幅提升，冗余调用减少

Conclusion: 使反思明确化并直接优化可提高工具交互的可靠性，为智能体从失败中学习提供可复现路径

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [77] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出Point Prompt Defender框架，通过对抗性强化学习自动优化SAM的点提示，采用攻击-防御范式提升分割性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或手动设计的提示，限制了可扩展性和泛化能力，需要自动化的提示优化方案

Method: 构建双空间图表示图像块，使用DQN训练攻击者和防御者智能体，攻击者学习破坏SAM性能的提示，防御者学习抑制这些提示

Result: 实验表明该方法有效提升SAM的鲁棒性和泛化能力，无需重新训练即可应用于多样化任务

Conclusion: Point Prompt Defender为基于提示的分割提供了灵活、可解释、即插即用的框架

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [78] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds是一个多模态野生动物监测数据集，包含无人机图像、相机陷阱照片/视频和生物声学记录，支持多模态AI研究用于环境监测和物种保护。


<details>
  <summary>Details</summary>
Motivation: 解决濒危物种研究、保护生态学和栖息地管理中多模态监测的关键需求，建立可复制的多模态野生动物监测协议。

Method: 在220英亩牧场进行为期四天的同步监测，收集三种模态数据（无人机图像、相机陷阱、生物声学记录），涵盖多种物种。

Result: 提供了传感器模态性能的比较分析，展示了不同模态在土地利用模式、物种检测、行为分析和栖息地监测方面的互补优势。

Conclusion: 该工作建立了可复制的多模态野生动物监测协议，并为保护计算机视觉研究贡献了开放数据集，未来将扩展包含GPS跟踪数据和更长时间覆盖。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [79] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 本文提出了RS3DBench基准数据集，包含54,951对遥感图像与像素级对齐的深度图，用于推动遥感图像3D视觉模型的发展，并基于稳定扩散提出了先进的深度估计模型。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集缺乏全面的深度信息或深度数据与遥感图像之间的精确对齐，这限制了3D视觉模型在遥感领域的发展。

Method: 构建了RS3DBench数据集，包含大量遥感图像-深度图对和文本描述；基于稳定扩散的多模态融合能力开发了遥感深度估计模型。

Result: 提出的深度估计模型在RS3DBench数据集上实现了最先进的性能。

Conclusion: 该工作将为遥感领域的3D视觉感知模型和地理人工智能的发展做出重要贡献，数据集、模型和代码将公开提供。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [80] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: DeblurSplat：首个无需SfM的基于事件相机的去模糊3D高斯泼溅方法，通过利用预训练的稠密立体模块直接获取初始点云，并引入事件流进行精细监督，实现高效高质量的去模糊3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 解决传统运动去模糊方法依赖SfM导致的相机位姿累积误差问题，以及缺乏对动态变化敏感监督信号的问题。

Method: 1. 利用DUSt3R稠密立体模块直接从模糊图像获取准确初始点云，避免SfM位姿误差传递；2. 引入事件流解码潜在清晰图像，为场景重建优化提供精细监督信号。

Result: 在多种场景下的实验表明，DeblurSplat不仅能生成高保真度的新视角图像，而且在渲染效率上显著优于现有的去模糊3D-GS方法。

Conclusion: 该方法成功实现了无需SfM的高效去模糊3D高斯泼溅重建，为运动模糊场景的3D重建提供了新的解决方案。

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [81] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: 提出了MoiréNet，一种基于U-Net的卷积神经网络框架，通过协同整合频域和空间域特征来有效去除图像中的莫尔条纹伪影。


<details>
  <summary>Details</summary>
Motivation: 莫尔条纹是由显示像素阵列和相机传感器网格之间的频谱混叠产生的各向异性、多尺度伪影，对数字图像去莫尔处理构成重大挑战。

Method: MoiréNet包含两个关键组件：方向频率空间编码器（DFSE）通过方向差分卷积识别莫尔条纹方向，以及频率空间自适应选择器（FSAS）实现精确的特征自适应抑制。

Result: 在公开和实际使用的数据集上的广泛实验表明，MoiréNet实现了最先进的性能，同时具有高参数效率。仅需5.513M参数，比ESDNet-L减少48%。

Conclusion: MoiréNet将卓越的恢复质量与参数效率相结合，非常适合资源受限的应用，包括智能手机摄影、工业成像和增强现实。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [82] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的频率感知音频-视觉分割(FAVS)框架，通过频率域分解和重组来解决音频-视觉分割中模态间频率域矛盾的问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉分割方法忽视了音频和视觉模态在频率域上的固有矛盾——音频高频信号普遍存在干扰噪声，而视觉高频信号包含丰富的结构细节。忽略这些差异会导致次优性能。

Method: FAVS框架包含两个关键模块：频率域增强分解器(FDED)模块和协同跨模态一致性(SCMC)模块。FDED采用基于残差的迭代频率分解来区分模态特定语义和结构特征，SCMC利用混合专家架构通过动态专家路由增强语义一致性和模态特定特征保留。

Result: 在三个基准数据集上的大量实验表明，FAVS框架实现了最先进的性能，丰富的定性可视化进一步验证了所提模块的有效性。

Conclusion: 将AVS任务重新定义为频率域分解和重组问题，提出的FAVS框架能够有效处理音频-视觉模态间的频率域矛盾，显著提升分割性能。

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [83] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 本文综述了可解释人工智能（xAI）在视觉感知任务中的四种代表性方法：显著性图、概念瓶颈模型、基于原型的方法和混合方法，分析了它们的机制、优缺点及评估指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像分析任务中表现出色，但模型决策过程不透明，难以解释，这在关键应用中存在可靠性问题。xAI旨在解决这一挑战，提供理解AI模型决策过程的方法。

Method: 本文采用综述方法，系统分析四种xAI方法：显著性图（可视化模型关注区域）、概念瓶颈模型（引入可解释概念层）、原型方法（基于典型样本解释）和混合方法（结合多种技术）。

Result: 通过对比分析，揭示了不同xAI方法的机制特点和适用场景，总结了各自的优势与局限性，并提供了评估指标框架。

Conclusion: xAI对于提高深度学习模型的可信度至关重要，未来研究需要开发更有效的解释方法并建立标准化评估体系，以推动在关键领域的应用。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [84] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 本文提出了一种基于去噪扩散概率模型（DDPM）的方法，通过改进的噪声调度和时间步嵌入技术生成高质量合成LiDAR数据，用于自动驾驶车辆的3D视觉系统增强。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖LiDAR数据进行环境感知，但真实LiDAR数据采集耗时且易受噪声和稀疏性影响。需要生成合成数据来提升计算机视觉任务性能。

Method: 使用改进的DDPM模型，包含新颖的噪声调度和时间步嵌入技术，能够生成更真实的点云数据。在IAMCV和KITTI-360数据集上进行评估。

Result: 该方法在四个性能指标上优于现有最先进方法，能有效减轻噪声和稀疏LiDAR数据的影响，生成具有丰富空间关系和结构细节的多样化点云。

Conclusion: 提出的方法在自动驾驶感知任务中表现出优越性能，为LiDAR数据增强提供了有效解决方案。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [85] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为AGSSP的新型预训练范式，通过异常先验指导表示学习，解决了金属表面缺陷检测中预训练-微调范式的关键困境。


<details>
  <summary>Details</summary>
Motivation: 传统的预训练方法面临两个问题：在自然图像数据集（如ImageNet）上预训练存在显著领域差距，而在工业数据上进行自监督预训练又因现有学习目标无法区分细微缺陷模式与复杂背景噪声而效果不佳。

Method: AGSSP采用两阶段框架：首先通过从异常图中蒸馏知识来预训练模型骨干网络，鼓励网络捕捉缺陷显著特征；然后使用从这些图中导出的伪缺陷框来预训练检测器，使其与定位任务对齐。还开发了知识增强方法来生成高质量异常图，并收集了12万张图像的大规模工业数据集。

Result: 大量实验表明，AGSSP在各种设置下都能持续提升性能，相比基于ImageNet的模型，在mAP@0.5上最高提升10%，在mAP@0.5:0.95上提升11.4%。

Conclusion: AGSSP有效解决了金属表面缺陷检测中的预训练困境，通过异常先验指导表示学习，显著提升了检测性能，所有代码、预训练模型和数据集均已公开。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [86] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 本文提出了首个音频驱动的通用逼真头像合成方法，结合了与人物无关的语音模型和新的通用头部头像先验（UHAP），能够同时处理几何变形和外观变化，实现高保真的唇部同步和表情细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将音频特征映射到几何变形，而忽略了音频相关的外观变化。本文旨在开发一个能够同时处理几何和外观变化的通用音频驱动头像模型。

Method: 使用跨身份多视角视频训练UHAP，通过中性扫描数据进行监督以捕捉身份特定细节。开发通用语音模型将原始音频直接映射到UHAP潜在表达空间，该空间编码了几何和外观变化。采用单目编码器进行高效个性化。

Result: 该方法能够生成高度逼真的头像，具有精确的唇部同步和细致的表情细节（如眉毛运动、视线转移和逼真的口腔内部外观）。在唇部同步准确性、定量图像质量和感知真实性方面优于现有方法。

Conclusion: 这是首个能够进行详细外观建模和渲染的通用音频驱动头像模型，在多个评估指标上均优于竞争对手的几何方法。

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [87] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 本文提出了一个基于机器学习的模块化管道，用于自动检测、跟踪和提取树突棘在3D+时间显微镜数据中的特征，解决了大规模分析树突棘结构动力学的挑战。


<details>
  <summary>Details</summary>
Motivation: 树突棘是大脑中兴奋性突触的关键结构组成部分，其大小可作为突触效能的指标。然而，在3D+时间显微镜数据中对树突棘结构动力学进行大规模分析仍然具有挑战性且劳动密集。

Method: 该方法采用模块化机器学习管道，包括基于transformer的检测模块、整合空间特征的深度跟踪组件、利用空间一致性关联3D树突棘的时间跟踪模块，以及量化生物相关脊柱特征的特征提取单元。

Result: 该方法在开源标记的脊柱数据上进行了验证，并发布了两个互补的注释数据集（检测和深度跟踪数据集、时间跟踪数据集），建立了可扩展的端到端树突棘动力学分析基准。

Conclusion: 该研究为树突棘动力学的大规模分析提供了一个自动化解决方案，通过发布数据、代码和预训练权重，为未来研究奠定了基础。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [88] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 提出一种结合视觉语言模型和预训练视觉模型的自学习零样本图像分类框架，无需标注数据，仅需类别名称即可训练轻量级分类器


<details>
  <summary>Details</summary>
Motivation: 深度学习依赖大量标注数据，但在实际场景中标注数据稀缺，需要解决零样本分类问题

Method: 使用置信度伪标注策略，VLM识别高置信度样本，预训练视觉模型增强视觉表示，迭代训练轻量级分类器

Result: 在十个不同数据集上的实验表明，该方法优于基线零样本方法

Conclusion: 该方法能够有效结合语义和视觉线索，无需VLM微调或大型语言模型，降低对语义表示的依赖

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [89] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 本文提出了MirrorScene3D数据集和ReflectiveGS方法，用于解决含有镜面环境的3D重建和新视角合成问题。通过利用镜面反射作为补充视角而非对称伪影，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法（如NeRF和3DGS）在含有镜面的场景中性能下降，现有解决方案主要关注镜面表面的对称映射，但忽视了镜面反射所携带的丰富信息，这些反射可以提供补充视角来填补缺失细节。

Method: 提出了ReflectiveGS方法，这是3D高斯泼溅的扩展，将镜面反射作为补充视角而非简单对称伪影来利用，从而增强场景几何并恢复缺失细节。同时构建了MirrorScene3D数据集作为基准。

Result: 在MirrorScene3D数据集上的实验表明，ReflectiveGS在SSIM、PSNR、LPIPS指标和训练速度上都优于现有方法。

Conclusion: ReflectiveGS为镜面丰富环境中的3D重建设定了新的基准，证明了利用镜面反射作为补充视角的有效性。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [90] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 使用深度学习（Yolo算法和GAN）从手术白光图像中定位胆道，以降低腹腔镜胆囊切除术中胆管损伤的风险。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术是常见手术，但存在较高的胆管损伤风险，严重影响患者生活质量和生存率。通过改善术中胆道可视化来避免损伤。

Method: 构建并标注图像数据库训练Yolo检测算法，采用经典数据增强技术，并使用生成对抗网络（GAN）生成部分合成训练数据集。

Result: 实验结果表明该方法能有效定位胆道，讨论了实验结果和伦理考量。

Conclusion: 深度学习技术可用于改善腹腔镜胆囊切除术中的胆道可视化，降低胆管损伤风险。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [91] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 提出了Prompt-DAS框架，这是一个基于SAM的可提示多任务框架，用于电子显微镜图像中的细胞器实例分割，支持无监督、弱监督和交互式分割。


<details>
  <summary>Details</summary>
Motivation: 解决大规模电子显微镜图像中细胞器实例分割的标注效率问题，通过域自适应学习减少对大量标注数据的依赖。

Method: 结合辅助中心点检测任务和提示引导对比学习，支持全点、稀疏点或无点提示配置，实现灵活的自适应训练和测试。

Result: 在多个挑战性基准测试中，该方法在无监督域适应、弱监督域适应和基于SAM的方法上均表现出优越性能。

Conclusion: Prompt-DAS框架提供了一种高效灵活的细胞器实例分割解决方案，显著提升了标注效率和分割性能。

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [92] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 本文提出了VIR-Bench基准测试，针对多模态大语言模型在长距离旅行视频理解方面的不足，通过200个旅行视频评估模型的时空智能，并开发了旅行规划代理验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前视频基准测试主要关注室内场景或短距离户外活动，缺乏对长距离旅行挑战的探索。掌握扩展的地理时空轨迹对于下一代MLLMs至关重要，支持现实世界任务如具身AI规划和导航。

Method: 构建VIR-Bench基准，包含200个旅行视频，将行程重建作为评估任务。通过实验评估现有MLLMs表现，并开发原型旅行规划代理验证基准的有效性。

Result: 实验结果显示，包括专有模型在内的最先进MLLMs都难以获得高分，表明处理跨越扩展时空尺度的视频具有挑战性。旅行规划代理的行程推荐显著改善，验证了评估协议的有效性。

Conclusion: VIR-Bench不仅有效评估模型性能，还能转化为用户面向应用中的具体性能提升，为推进MLLMs的时空智能提供了重要基准。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [93] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉语言模型的逐步推理框架CoS，通过细粒度的步骤级推理和过程奖励模型，实现了有效的强化学习和推理时扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链推理方法在视觉语言推理中主要采用粗粒度的推理链，难以进行细粒度结构化推理，且难以评估中间推理的质量和奖励。

Method: 提出了一个简单有效的框架，包括步骤级推理数据、过程奖励模型（PRM）和强化学习训练，实现细粒度的推理质量评估。

Result: 在具有挑战性的视觉语言基准测试中取得了持续改进的强基线结果，并通过消融研究揭示了各组件的影响和推理时扩展的有趣特性。

Conclusion: 该研究为视觉语言模型建立了基准，并为更复杂的多模态推理提供了见解，相关数据集、PRM和代码已开源。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [94] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 提出了一种基于SAM和ViT的弱监督食物图像语义分割方法，利用ViT的类激活图生成SAM提示，无需像素级标注即可实现食物分割。


<details>
  <summary>Details</summary>
Motivation: 解决食物图像分割中像素级标注成本高的问题，利用SAM的零样本能力和ViT的注意力机制实现弱监督分割。

Method: 使用Swin Transformer生成类激活图作为SAM的提示，结合图像预处理技术和单/多掩码生成策略，在FoodSeg103数据集上验证。

Result: 平均每张图像生成2.4个掩码（不含背景），多掩码场景下mIoU达到0.54。

Conclusion: 该方法可作为食物图像标注的加速工具，或集成到食物营养追踪应用中。

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [95] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet是一种基于动态学习的时序一致性U-Net分割架构，旨在实现超声心动图分割的时间稳定性和精确性，通过构建回声动态图和心脏相位动态注意力机制来解决帧间分割抖动问题。


<details>
  <summary>Details</summary>
Motivation: 超声心动图在心血管诊断和治疗中至关重要，但易受变形和斑点噪声影响，导致帧间分割抖动。即使单帧分割精度高，时间不稳定性也会削弱功能估计并影响临床可解释性。

Method: 提出DyL-UNet框架，通过动态学习构建回声动态图(EDG)提取视频动态信息。采用多个基于Swin-Transformer的编码器-解码器分支处理单帧图像，并在跳跃连接处引入心脏相位动态注意力(CPDA)，利用EDG编码的动态特征和心脏相位线索来增强分割的时间一致性。

Result: 在CAMUS和EchoNet-Dynamic数据集上的广泛实验表明，DyL-UNet在保持与现有方法相当的分割精度的同时，实现了更优的时间一致性。

Conclusion: DyL-UNet为自动化临床超声心动图提供了可靠的解决方案，能够实现时间稳定且精确的分割。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [96] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: ColorBlindnessEval是一个新颖的基准测试，用于评估视觉语言模型在受石原色盲测试启发的视觉对抗场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在复杂视觉环境中的鲁棒性存在不足，特别是在处理对抗性视觉模式时容易产生幻觉问题。

Method: 构建包含500张石原色盲测试风格图像的数据集，数字范围0-99，使用是/否和开放式提示评估9个VLM模型，并与人类参与者进行对比。

Result: 实验显示模型在对抗性环境中识别数字的能力存在明显局限，普遍出现幻觉问题，性能不如人类参与者。

Conclusion: 该基准测试为评估和改进VLM在现实应用中的可靠性提供了有价值的工具，强调了提升模型在复杂视觉环境中鲁棒性的必要性。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [97] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian是一个用于稀疏视图3D高斯对象重建的高效框架，通过在小波域应用扩散模型来显著减少训练时间，同时保持竞争性的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在稀疏视图设置下性能急剧下降，现有方法使用扩散模型修复损坏的渲染，但计算成本高昂。

Method: 将扩散转移到小波域：仅对低分辨率LL子带应用扩散，而高频子带使用轻量级网络细化；提出高效的在线随机掩码策略来构建训练对。

Result: 在Mip-NeRF 360和OmniObject3D两个基准数据集上的实验表明，WaveletGaussian实现了竞争性的渲染质量，同时大幅减少了训练时间。

Conclusion: WaveletGaussian通过小波域扩散和高效训练策略，在稀疏视图3D高斯重建中实现了计算效率和渲染质量的良好平衡。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [98] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i是对Sa2VA模型的改进版本，通过修复训练和推理过程中的不一致性问题，在多个视频分割基准上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 发现Sa2VA模型在视频对象分割任务中未能发挥其全部潜力，主要原因是训练和推理过程存在不一致性。

Method: 提出Sa2VA-i模型，通过修正训练和推理过程中的不一致性问题来改进原Sa2VA模型。

Result: Sa2VA-i在多个视频基准上创造了新的最先进结果：MeViS提升+11.6 J&F，Ref-YT-VOS提升+1.4，Ref-DAVIS提升+3.3，ReVOS提升+4.1。Sa2VA-i-1B模型在MeViS基准上甚至能与原Sa2VA-26B模型相媲美。

Conclusion: 这项工作强调了看似微不足道的实现细节的重要性，为视频分割领域提供了有价值的见解。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [99] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出一种无需训练的零样本方法，将多光谱数据输入到仅训练RGB数据的通用多模态模型中，实现遥感图像分类


<details>
  <summary>Details</summary>
Motivation: 解决多光谱遥感图像分析需要专门训练模型的高成本问题，以及通用多模态模型无法处理多光谱信号的限制

Method: 利用多模态模型对视觉空间的理解，将多光谱数据适配到该空间，并将领域特定信息作为指令注入模型，使用Gemini2.5模型进行验证

Result: 在流行的遥感基准测试中，该方法在土地覆盖和土地利用分类任务上表现出强大的零样本性能提升

Conclusion: 该方法展示了地理空间专业人员可以轻松利用强大的多模态模型来处理非标准专业输入数据，加速工作流程

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [100] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V是一个多模态医学基础模型，结合图像分析和文本推理，在单一框架中实现病变定位、结构化报告生成和医生级诊断推理。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像模型过于专业化，需要多个专用网络，限制了泛化能力。临床应用需要精确的视觉定位、多模态整合和链式推理能力。

Method: 提出新颖的多模态训练方法，整合检测、分割和多模态链式推理，发布涵盖推理、检测、分割和文档理解任务的开放数据套件。

Result: Citrus-V在多个基准测试中优于现有开源医学模型和专家级影像系统，支持精确病变量化、自动化报告和可靠第二意见。

Conclusion: 该模型提供了从视觉定位到临床推理的统一流程，为临床诊断和治疗决策提供强大支持。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 本研究探索多模态大语言模型在零样本设置下基于基础设施摄像头图像检测和描述交通事故的能力，通过集成视觉分析技术提升模型性能，展示了在真实交通监控系统中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 交通事故检测对公共安全至关重要，但缺乏多样化、真实的基础设施事故数据。研究旨在利用MLLMs减少对大量标注数据的依赖，实现自动化的实时事故监测。

Method: 使用CARLA模拟的DeepAccident数据集评估MLLMs；比较Gemini 1.5/2.0、Gemma 3和Pixtral模型；集成YOLO、Deep SORT和SAM等视觉分析技术增强提示效果。

Result: Pixtral表现最佳（F1-score 0.71，召回率83%）；Gemini模型通过增强提示精度提升至90%，但F1和召回率下降；Gemma 3性能最平衡。

Conclusion: MLLMs与先进视觉分析技术结合在交通事故检测中具有显著潜力，可增强真实世界自动化交通监控系统的适用性。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2是一个基于Transformer的在线长期点跟踪模型，通过架构改进、内存优化和合成训练策略提升性能，在多个基准测试中达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 解决长期点跟踪问题，需要在显著外观变化、运动和遮挡下实现跨视频帧的一致点识别，并针对在线实时应用场景

Method: 扩展Track-On模型，采用因果处理框架和内存机制维持时间一致性，通过粗粒度补丁级分类和细化进行推理，系统研究合成训练对内存行为的影响

Result: 在五个合成和真实世界基准测试中取得最先进结果，超越现有在线跟踪器甚至利用双向上下文的离线方法

Conclusion: 基于因果处理和内存架构、纯合成数据训练的模型是解决真实世界点跟踪问题的可扩展有效方案

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA是一个用于多相机、多光谱同步和实时检测海豹与北极熊的综合系统，可将数据集处理时间减少80%


<details>
  <summary>Details</summary>
Motivation: 为阿拉斯加周边海域的空中调查提供高效的海豹和北极熊检测解决方案，解决传统方法处理时间长的问题

Method: 采用严格的校准和硬件同步技术，利用多光谱进行目标检测，所有收集的数据都带有元数据注释，并将图像和检测结果映射到世界平面

Result: 系统在Bering、Chukchi和Beaufort海域的空中调查中应用，处理时间比之前方法减少80%，能够准确估计调查区域并快速评估调查结果

Conclusion: KAMERA系统有望推动科学界的其他测绘和检测工作，所有软件、模型和原理图都已完全开源

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX是一种神经形态协同推理架构，通过联合优化空间和时间冗余，显著减少数据传输和边缘能耗，在保持精度的同时实现高效的脉冲神经网络边缘部署。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在边缘计算中面临延迟和能耗挑战，传统的边缘-云协同推理系统存在高延迟和特征传输成本问题，需要一种更高效的解决方案。

Method: NeuCODEX结合了学习驱动的脉冲压缩模块来减少数据传输，并采用动态提前退出机制根据输出置信度自适应终止推理。

Result: 在CIFAR10、Caltech等数据集上测试，NeuCODEX可将数据传输减少2048倍，边缘能耗降低90%以上，端到端延迟减少3倍，精度损失小于2%。

Conclusion: NeuCODEX为资源受限环境中的脉冲神经网络部署提供了实用高效的技术方案，显著提升了边缘计算的可行性和性能。

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 提出RoSe方法，通过引入视觉基础模型的鲁棒先验和场景对应先验，改进自监督立体匹配在恶劣天气条件下的性能


<details>
  <summary>Details</summary>
Motivation: 现有自监督立体匹配方法在恶劣天气条件下性能显著下降，主要问题包括CNN特征提取器在退化区域表现不佳，以及光度一致性假设在恶劣天气下失效

Method: 1) 从视觉基础模型注入鲁棒先验到CNN特征提取器；2) 引入场景对应先验构建鲁棒监督信号；3) 创建包含清晰和恶劣天气图像对的合成数据集；4) 提出鲁棒自监督训练范式，包括场景对应学习和恶劣天气蒸馏

Result: 大量实验证明该方法有效且通用，在恶劣天气条件下优于现有最先进的自监督方法

Conclusion: RoSe方法通过结合视觉基础模型的鲁棒先验和场景对应先验，显著提升了自监督立体匹配在恶劣天气条件下的性能

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: 提出了YOLO-LAN，一种基于YOLO的息肉检测管道，通过M2IoU损失、数据增强和负数据训练，在Kvasir-seg和BKAI-IGH NeoPolyp数据集上表现优于现有方法，实现了高精度的实时息肉检测。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是一种致命疾病，始于结肠内壁异常黏膜细胞增殖形成的息肉。传统结肠镜检查存在不一致性和漏检问题，需要更准确、实时的AI辅助检测方案。

Method: 开发了YOLO-LAN息肉检测管道，采用YOLO架构，使用M2IoU损失函数、多样化数据增强技术和负数据训练，以模拟真实临床场景。

Result: 在Kvasir-seg数据集上，YOLOv12达到mAP50为0.9619，mAP50:95为0.8599；YOLOv8达到mAP50为0.9540，mAP50:95为0.8487，显著提升了检测精度。

Conclusion: 该方法在息肉大小和精确定位检测方面表现出鲁棒性，具有临床相关性，可用于AI辅助结直肠癌筛查。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 该技术报告分析了LSVOS挑战赛中的MOSEv2赛道，通过改进SeC（增强版SAM-2框架）的长时记忆和概念感知记忆机制，在复杂半监督视频对象分割任务中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过长时记忆和概念感知记忆机制来解决半监督视频对象分割中的遮挡、重现和干扰物等核心挑战。

Method: 分析和改进SeC框架，重点研究其长时记忆（保持时间连续性）和概念感知记忆（提供语义先验抑制干扰物）的协同作用。

Result: 在MOSEv2测试集上获得39.89%的JF分数，在LSVOS挑战赛MOSEv2赛道中排名第一。

Conclusion: 长时记忆和概念感知记忆的结合能有效提升半监督视频对象分割性能，特别是在处理遮挡、重现和干扰物等挑战性场景时表现出色。

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 该论文分析了视觉语言模型(VLMs)的视觉处理机制，基于人类视觉的双流假设将视觉处理解构为物体识别和空间感知两个独立部分进行研究，并提出了提升解码效率和空间推理能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs通过序列化图像处理视觉信息，与人类视觉的并行处理方式存在差异，且其内部机制不透明，阻碍了深度理解和架构创新。受人类视觉双流假设启发，研究者希望深入理解VLM的视觉处理机制。

Method: 将VLM视觉处理解构为物体识别和空间感知：1）物体识别方面，将图像转换为文本标记图，分析模型从浅层到深层的两阶段处理过程；2）空间感知方面，理论推导和实证验证VLM位置表示的几何结构。基于发现提出了指令无关的标记压缩算法和RoPE缩放技术。

Result: 研究发现VLM的视觉感知呈现从属性识别到语义消歧的两阶段过程，并验证了位置表示的几何结构。提出的方法有效提升了解码效率和空间推理能力。

Conclusion: 该工作为理解VLM内部机制提供了深刻见解，并为设计更强大的未来架构提供了明确原则，验证了基于人类视觉双流假设的分析框架的有效性。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 本文提出了一种无视觉编码器的检索方法，通过使用VLLM生成的结构化图像描述，将传统的文本-图像检索范式转变为文本-文本检索范式，解决了传统视觉语言模型存在的模态鸿沟、组合性差和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统的对比训练视觉语言模型（如CLIP）存在浅层语言理解、模态鸿沟、计算成本高和隐私问题等局限性，需要一种更高效、隐私友好的替代方案。

Method: 采用无视觉编码器的单编码器检索流程，利用VLLM生成结构化图像描述，将检索任务从文本-图像转换为文本-文本范式，仅需少量GPU时间进行校准。

Result: 该方法显著减少了模态鸿沟，提高了组合性，在短长标题查询上表现更好，且在多个检索和组合性基准测试中实现了最先进的零样本性能，模型参数仅需0.3B。

Conclusion: 无视觉检索器在性能和隐私保护方面均优于传统多模态模型，展示了文本-文本检索范式的有效性，并发布了新的组合性基准测试subFlickr和subCOCO。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 对比视觉语言模型在理解长而密集的标题方面存在挑战，研究发现组合性训练和长标题理解之间存在双向促进关系，但效果受数据质量和模型设计影响。


<details>
  <summary>Details</summary>
Motivation: 解决对比视觉语言模型在理解长、密集标题方面的局限性，探索组合性（对象-属性绑定和对象间关系推理能力）与长标题理解之间的相互作用。

Method: 训练和评估一系列针对组合性和长标题理解能力的模型，分析不同训练策略（如冻结位置嵌入）和数据质量对性能的影响。

Result: 发现组合性训练能提高长标题检索性能，长标题训练也能促进组合性理解，但这种增益对数据质量和模型设计敏感。高质量长标题数据训练可获得双重任务强性能。

Conclusion: 组合性理解和长标题理解是相互关联的能力，可以通过在密集、基础描述上进行训练来共同学习，为改进VLM泛化提供实用指导。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 提出一个结合合成RGB图像、少量真实标注和GAN跨模态对齐的框架，用于提升热图像中的植物语义分割性能，在复杂田间环境中显著改善分割效果


<details>
  <summary>Details</summary>
Motivation: 热图像中的植物分割在户外高通量表型分析中面临挑战，主要是植物与杂草对比度低和频繁遮挡导致性能不佳

Method: 使用1,128张合成图像训练模型生成作物和杂草分割掩码，结合少量真实标注图像，通过CycleGAN-turbo实现RGB到热图像的跨模态对齐

Result: 结合所有合成图像和少量真实图像，杂草类和植物类的分割性能分别相对提升了22%和17%

Conclusion: 合成数据与有限手动标注结合，通过生成模型的跨域翻译能显著提升复杂田间环境中多模态图像的分割性能

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: 本文介绍了HyKid数据集，这是一个针对儿童脑积水的开源神经影像数据集，包含48名患者的3D MRI图像和专家标注的脑组织分割，特别关注脉络丛分割，并展示了脉络丛体积与脑脊液总量的强相关性可作为脑积水评估的生物标志物。


<details>
  <summary>Details</summary>
Motivation: 儿童脑积水评估面临挑战，现有研究缺乏公开的专家标注数据集，特别是包含脉络丛分割的数据集。

Method: 开发了HyKid数据集，包含48名儿科脑积水患者的3D MRI图像（1mm各向同性分辨率），使用切片到体积算法从常规低分辨率图像重建，由经验丰富的神经学家手动校正脑组织分割，并通过检索增强生成框架从临床放射学报告中提取结构化数据。

Result: 脉络丛体积与总脑脊液体积之间存在强相关性，在预测模型中表现出色（AUC = 0.87），为脑积水评估提供了潜在的生物标志物。

Conclusion: HyKid数据集为神经影像算法开发提供了高质量基准，并揭示了脉络丛相关特征在脑积水评估中的重要性，数据集已公开可用。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 提出了一种多尺度特征交互网络(MsFIN)用于从行车记录仪视频中早期预测事故，通过多尺度特征聚合、时序特征处理和后融合来解决交通参与者特征交互和复杂多时序行为建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着行车记录仪的广泛部署和计算机视觉技术的发展，从行车记录仪视角开发事故预测模型对于主动安全干预变得至关重要。但存在两个关键挑战：建模交通参与者之间的特征级交互（在行车记录仪视图中经常被遮挡）和捕捉事故前复杂、异步的多时序行为线索。

Method: MsFIN包含三个层次：多尺度特征聚合层使用多尺度模块提取短期、中期和长期时间尺度的场景表示，并利用Transformer架构促进全面特征交互；时序特征处理层在因果约束下捕捉场景和对象特征的序列演化；多尺度特征后融合阶段融合多个时间尺度的场景和对象特征生成综合风险表示。

Result: 在DAD和DADA数据集上的实验表明，MsFIN在预测准确性和及时性方面显著优于采用单尺度特征提取的最先进模型。消融研究验证了MsFIN中每个模块的有效性。

Conclusion: MsFIN通过多尺度特征融合和上下文交互建模实现了优越的性能，为行车记录仪视角的事故早期预警提供了有效解决方案。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种基于持续学习的数字人脸伪造检测方法，采用发展性专家混合架构，通过Real-LoRA和Fake-LoRAs分别学习真实人脸和不断演变的伪造类型，有效解决模型遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 数字人脸生成和篡改技术的快速发展给社会带来严重风险，现有检测模型难以跟上不断演变的伪造技术。需要让模型能够快速适应新领域，同时避免遗忘已学习的伪造类型。

Method: 使用发展性专家混合架构，以LoRA模型作为专家。分为Real-LoRA学习真实人脸知识，多个Fake-LoRAs捕获不同伪造类型的增量信息。通过正交梯度和正交损失防止梯度干扰和灾难性遗忘。

Result: 在数据集和篡改类型增量协议下的实验结果表明该方法具有有效性。

Conclusion: 将人脸伪造检测构建为持续学习问题，提出的方法能够有效应对不断演变的伪造技术，同时保持对已学习伪造类型的检测能力。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一个统一的多模态掩码扩散模型，支持图像理解和生成任务，具备物体定位、图像编辑和高分辨率图像合成等新能力，并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散语言模型如MMaDa和Muddit仅支持简单的图像级理解任务和低分辨率图像生成，缺乏高级功能如物体定位和高质量图像合成。

Method: 采用弹性混合Transformer架构、通用文本条件化和分层采样等新技术，通过规划和迭代自反思利用理解能力提升图像生成和编辑效果。

Result: 在RefCOCO物体定位、GenEval文本到图像生成和ImgEdit图像编辑等基准测试中超越Qwen2.5-VL和FluxKontext-dev等现有自回归和连续扩散模型，并在推理时提供显著加速。

Conclusion: Lavida-O是首个统一的掩码扩散模型，成功将图像理解能力用于提升生成和编辑质量，实现了多模态任务的全面突破。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: 提出了基于概念的视频相似性估计任务(ConViS)，通过预定义的关键语义概念来计算视频对之间的可解释相似性分数，支持人类化的视频相似性推理和概念条件视频检索。


<details>
  <summary>Details</summary>
Motivation: 人类能够从不同方面比较视频相似性，但现有模型依赖全局相似度分数，缺乏对多维度相似性的理解能力。大型多模态模型为利用自然语言进行视频比较任务提供了新机会。

Method: 引入ConViS任务框架，创建ConViS-Bench基准数据集，包含精心标注的视频对，每个视频对都有概念级相似性分数以及差异和相似性的文本描述。

Result: 在ConViS基准上对多个最先进模型进行基准测试，结果显示不同概念对视频相似性估计的挑战程度存在显著差异。

Conclusion: ConViS-Bench将成为推进语言驱动视频理解研究的宝贵资源，有助于开发更符合人类判断的视频相似性评估方法。

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 提出了一种基于对抗性精炼的VQ-GAN框架，通过密集运动标记化来压缩时空热图，有效解决人体运动重建中的运动模糊和时间错位问题。


<details>
  <summary>Details</summary>
Motivation: 连续人体运动理解在计算机视觉中面临高维度和冗余性的挑战，需要高效的压缩和表示方法来分析复杂运动动态。

Method: 结合密集运动标记化和对抗性精炼的VQ-GAN框架，通过密集标记化压缩运动数据，对抗性训练消除重建伪影。

Result: 在CMU Panoptic数据集上，方法比dVAE基线SSIM提升9.31%，时间不稳定性降低37.1%。发现2D运动可用128标记词汇表表示，3D运动需要1024标记码本。

Conclusion: 该方法为多样化运动分析应用提供了实际部署可行性，证明了密集标记化在运动复杂性分析中的有效性。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 本文提出了一种新的图放射组学学习（GrRAiL）描述符，用于在临床MRI扫描中表征病灶内异质性，通过图论方法量化病灶内空间关系，在区分肿瘤复发与放射性效应方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统放射组学方法在聚合整个感兴趣区域特征时会丢失复杂的空间关系，难以可靠区分恶性肿瘤与混淆病理。需要一种能够捕捉病灶内异质性空间关联的新方法。

Method: GrRAiL方法：1）使用逐体素放射组学测量识别亚区域聚类；2）计算图论指标量化聚类间的空间关联；3）生成加权图编码ROI内高阶空间关系。

Result: 在947名患者的多中心研究中，GrRAiL在三个应用场景中均显著优于基线方法：胶质母细胞瘤（测试准确率78%，提升>10%）、脑转移瘤（74%，提升>13%）、胰腺IPMN风险分层（75%，提升>10%）。

Conclusion: GrRAiL能够有效捕捉病灶内异质性的空间模式，在区分恶性肿瘤与放射性效应方面表现出优越性能，具有临床可行性。

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS是首个仅使用自我中心视觉来感知环境和导航的人类化身，通过将低级运动技能学习与高级视觉控制解耦，实现了人类化的运动特性


<details>
  <summary>Details</summary>
Motivation: 当前的人类运动生成方法忽视了感知与运动的相互依赖关系，使用与人类感知完全不同的任务特定"感知"。作者认为生成人类化化身行为需要人类化的感知

Method: 1. 在大规模运动捕捉数据集上训练运动先验模型；2. 使用Q学习训练策略，将自我中心视觉输入映射到运动先验的高级控制命令

Result: 实验证明自我中心视觉能够使化身产生人类化的运动特性，例如化身会根据视觉场中的障碍物调整行走路径以避免碰撞

Conclusion: 为化身配备人类化传感器，特别是自我中心视觉，有望训练出行为像人类的化身

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 该论文针对布局到图像生成中边界框重叠问题，提出了OverLayScore指标和OverLayBench基准，并开发了CreatiLayout-AM模型来改善重叠区域的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法在处理边界框显著重叠时表现不佳，特别是在大重叠区域和语义区分度小的重叠实例上。现有基准存在偏向简单情况的偏差，无法有效评估模型在挑战性条件下的性能。

Method: 1. 提出OverLayScore指标量化边界框重叠复杂度；2. 构建OverLayBench基准，包含高质量标注和平衡的OverLayScore分布；3. 开发CreatiLayout-AM模型，在精心策划的amodal掩码数据集上进行微调。

Result: 分析显示现有基准偏向低OverLayScore的简单情况。OverLayBench提供了更全面的评估框架，CreatiLayout-AM模型在复杂重叠场景下表现出改进的生成能力。

Conclusion: 该研究为在现实和挑战性场景下实现更鲁棒的布局到图像生成奠定了基础，通过新指标、基准和模型方法系统解决了重叠边界框带来的生成质量问题。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 提出了一种自蒸馏框架，将视频扩散模型中的隐式3D知识蒸馏到显式的3D高斯泼溅表示中，无需多视图训练数据即可生成3D场景


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的3D重建方法依赖真实世界的多视图数据，但这些数据并不总是可用。视频扩散模型具有强大的想象力，但其2D特性限制了在机器人导航等需要3D环境交互的应用

Method: 在典型的RGB解码器基础上增加3DGS解码器，通过RGB解码器的输出进行监督训练。3DGS解码器可以完全使用视频扩散模型生成的合成数据进行训练

Result: 实验结果显示，该框架在静态和动态3D场景生成方面达到了最先进的性能

Conclusion: 该框架能够从文本提示或单张图像实时合成3D场景，并可扩展到从单目输入视频生成动态3D场景

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat提出了一种新的多视图前馈3D高斯溅射方法，用体素对齐的高斯预测替代传统的像素对齐方法，解决了现有方法对输入视图数量的依赖、视图偏差密度分布和对齐误差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素对齐的高斯预测方法存在三个主要问题：重建的3D模型严重依赖输入视图数量、产生视图偏差的密度分布、在源视图存在遮挡或低纹理时引入对齐误差。

Method: VolSplat采用体素对齐的高斯预测范式，直接从预测的3D体素网格预测高斯分布，避免了像素对齐方法对易出错的2D特征匹配的依赖，确保多视图一致性。

Result: 在RealEstate10K和ScanNet等基准测试中，VolSplat实现了最先进的性能，产生更合理和视图一致的高斯重建，渲染质量得到提升。

Conclusion: 该方法不仅提供了优越的结果，还为前馈3D重建建立了更可扩展的框架，为更广泛社区的研究铺平了道路。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow是一种轻量级的条件感知重参数化方法，通过调整源分布和目标分布来缩短流匹配模型需要学习的概率路径，从而加速训练并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散和流的方法需要模型同时学习质量传输和条件注入，这增加了模型的学习负担。为了减轻模型负担，作者提出了条件感知重参数化方法。

Method: CAR-Flow通过学习一个轻量级的偏移量来调整源分布（标准高斯噪声）和目标分布（条件数据分布），缩短了概率路径的长度。该方法可以应用于源分布、目标分布或两者同时调整。

Result: 在低维合成数据上可视化验证了CAR的效果。在ImageNet-256数据集上，将CAR-Flow应用于SiT-XL/2模型，FID从2.07降至1.68，仅增加不到0.6%的参数。

Conclusion: CAR-Flow通过简单的重参数化策略有效提升了流匹配模型的训练效率和生成质量，是一种高效的条件生成建模方法。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>
